---
title: Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey
source: https://arxiv.org/html/2408.16202v1
scraped_date: 2026-01-20 10:28:10
---

# Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey

Qi Dong  [0009-0008-1737-9324](https://orcid.org/0009-0008-1737-9324 "ORCID identifier") [3230006098@student.must.edu.mo](mailto:3230006098@student.must.edu.mo) School of Computer Science and Engineering, Macau University of Science and TechnologyTaipaMacauChina999078 ,Â  Rubing Huang  [rbhuang@must.edu.mo](mailto:rbhuang@must.edu.mo) [0000-0002-1769-6126](https://orcid.org/0000-0002-1769-6126 "ORCID identifier") School of Computer Science and Engineering, Macau University of Science and TechnologyTaipaMacauChina999078 ,Â  Chenhui Cui  [3230002105@student.must.edu.mo](mailto:3230002105@student.must.edu.mo) [0009-0004-8746-316X](https://orcid.org/0009-0004-8746-316X "ORCID identifier") School of Computer Science and Engineering, Macau University of Science and TechnologyTaipaMacauChina999078 ,Â  Dave Towey  [dave.towey@nottingham.edu.cn](mailto:dave.towey@nottingham.edu.cn) [0000-0003-0877-4353](https://orcid.org/0000-0003-0877-4353 "ORCID identifier") School of Computer Science, University of Nottingham Ningbo ChinaNingboZhejiangChina315100 ,Â  Ling Zhou  [lzhou@must.edu.mo](mailto:lzhou@must.edu.mo) [0000-0002-8313-5749](https://orcid.org/0000-0002-8313-5749 "ORCID identifier") School of Computer Science and Engineering, Macau University of Science and TechnologyTaipaMacauChina999078 ,Â  Jinyu Tian  [jytian@must.edu.mo](mailto:jytian@must.edu.mo) [0000-0002-2449-5277](https://orcid.org/0000-0002-2449-5277 "ORCID identifier") School of Computer Science and Engineering, Macau University of Science and TechnologyTaipaMacauChina999078 Â andÂ  Jianzhou Wang  [jzwang@must.edu.mo](mailto:jzwang@must.edu.mo) [0000-0001-9078-7617](https://orcid.org/0000-0001-9078-7617 "ORCID identifier") Department of Engineering Science, Macau University of Science and TechnologyTaipaMacauChina999078

(2018; )

###### Abstract.

Short-Term Electricity-Load Forecasting (STELF) refers to the prediction of the immediate demand (in the next few hours to several days) for the power system. Various external factors, such as weather changes and the emergence of new electricity consumption scenarios, can impact electricity demand, causing load data to fluctuate and become non-linear, which increases the complexity and difficulty of STELF. In the past decade, deep learning has been applied to STELF, modeling and predicting electricity demand with high accuracy, and contributing significantly to the development of STELF. This paper provides a comprehensive survey on deep-learning-based STELF over the past ten years. It examines the entire forecasting process, including data pre-processing, feature extraction, deep-learning modeling and optimization, and results evaluation. This paper also identifies some research challenges and potential research directions to be further investigated in future work. 

electricity, load, deep learning, short term 

â€ â€ copyright: acmcopyrightâ€ â€ journalyear: 2018â€ â€ doi: XXXXXXX.XXXXXXXâ€ â€ journal: JACMâ€ â€ journalvolume: 37â€ â€ journalnumber: 4â€ â€ article: 111â€ â€ publicationmonth: 8â€ â€ ccs: Computing methodologiesÂ Machine learning algorithmsâ€ â€ ccs: Mathematics of computingÂ Probability and statisticsâ€ â€ ccs: Applied computingÂ Forecasting

##  1\. Introduction

Electricity-Load Forecasting (ELF) aims to meet power systemsâ€™ daily operational, management, and planning needs. This can provide essential guidance and reference points for system operators and planners. The absence of large-scale energy storage technologies means that power systems must ensure a constant power supply to meet current demandsÂ (PeÅ‚ka and Dudek, [2020](https://arxiv.org/html/2408.16202v1#bib.bib135)). This means that ELF has become an essential component in the planning, scheduling, and operational management of power systems. Short-Term Electricity-Load Forecasting (STELF) uses historical load data to predict future loads, over a period ranging from several hours to a few days. STELF is a time-series forecasting task primarily used for the short-term scheduling of smart grids (including equipment maintenance, load distribution, and unit startup and shutdown), and for determining electricity pricesÂ (Wu etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib177)). Data from a British electricity company in 1984 showed that a 1% reduction in forecasting error could save Â£10 million annuallyÂ (Bunn and Farmer, [1985](https://arxiv.org/html/2408.16202v1#bib.bib19)). Increasing the STELF accuracy could improve planning and scheduling, and reduce operational costs for power systemsÂ (Kouhi etÂ al., [2014](https://arxiv.org/html/2408.16202v1#bib.bib103)). This practical impact of STELF has led to an increasing amount of attention from researchers. Our review of the literature from the past decade highlights that most load forecasting articles have focused on STELFÂ (Ghofrani etÂ al., [2015](https://arxiv.org/html/2408.16202v1#bib.bib58); Hu etÂ al., [2017](https://arxiv.org/html/2408.16202v1#bib.bib84); Raza etÂ al., [2017](https://arxiv.org/html/2408.16202v1#bib.bib139); Hoori etÂ al., [2019](https://arxiv.org/html/2408.16202v1#bib.bib77); Choi etÂ al., [2018](https://arxiv.org/html/2408.16202v1#bib.bib31); Guo etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib62)).

Many factors can impact the electricity load, including climate, weather, economic conditions, seasonality, and electricity prices. Furthermore, advances in smart-grid technologies, and the widespread adoption of smart meters and other sensors have significantly increased both the complexity and the volume of electricity-load data. The data exhibits strong non-linearity, randomness, volatility, and complexity. This is a challenge for STELF. An accurate, robust, and fast STELF model is essential for the reliable daily operations of power systemsÂ (Wang etÂ al., [2019c](https://arxiv.org/html/2408.16202v1#bib.bib171)), and the development of efficient forecasting models has become an important STELF research goal (Dou etÂ al., [2018](https://arxiv.org/html/2408.16202v1#bib.bib42)).

ELF has been extensively studied since the 1970s, with various methods having been proposedÂ (Fahiman etÂ al., [2019](https://arxiv.org/html/2408.16202v1#bib.bib46)). STELF methods can be broadly categorized into three types: statistical methods, machine learning methods, and deep learning methodsÂ (Dudek, [2016](https://arxiv.org/html/2408.16202v1#bib.bib43)). Statistical methods perform well when dealing with linear relationships, but are often inadequate for handling the nonlinear patterns commonly found in electricity-load data. Machine learning methods, such as support vector machines and decision trees, typically perform well with simple or moderately complex data patterns. Deep learning methods can capture and model complex nonlinear relationships through their multi-layered structures, which is particularly important for predicting dynamic changes in electricity loads. In summary, while traditional statistical and machine learning methods have their strengths, deep learning techniques are more suited to the dynamic and nonlinear characteristics of electricity-load data.

Deep learning approaches are among the most revolutionary breakthroughs in the fields of computer science and artificial intelligence in recent yearsÂ (LeCun etÂ al., [2015](https://arxiv.org/html/2408.16202v1#bib.bib106)). The concept of deep learning builds on earlier work on Artificial Neural Networks (ANNs)Â (Hinton etÂ al., [2006](https://arxiv.org/html/2408.16202v1#bib.bib72)), which were a type of shallow learning model Â (Schmidhuber, [2015](https://arxiv.org/html/2408.16202v1#bib.bib144)), consisting of an input layer, a hidden layer, and an output layerÂ (Almalaq and Edwards, [2017](https://arxiv.org/html/2408.16202v1#bib.bib7)). ANNs were used in early STELF studies (Hayati and Shirvany, [2007](https://arxiv.org/html/2408.16202v1#bib.bib68); Park etÂ al., [1991](https://arxiv.org/html/2408.16202v1#bib.bib134)). Deep Neural Networks (DNNs), a type of ANN with multiple hidden layers, can also be used for STELFÂ (Hosein and Hosein, [2017](https://arxiv.org/html/2408.16202v1#bib.bib78); Lai etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib104); Alipour etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib6)). The multiple hidden layers in DNNs enable a complex computational framework that uses features as inputs to represent different levels of data abstraction. Through a cascading network structure, each layer in the DNN is capable of extracting and recognizing different features of the data, forming a hierarchy from basic to advanced features, thereby significantly enhancing both the modelâ€™s flexibility and its ability to handle complex issues. Recurrent Neural Networks (RNNs)Â (Rumelhart etÂ al., [1986](https://arxiv.org/html/2408.16202v1#bib.bib140)) are DNNs that were designed specifically to process sequential data, making them highly suitable for time-series prediction tasks. Although RNNs are theoretically ideal for time-series data, they may face challenges, like vanishing or exploding gradients in practical applicationsÂ (Tang etÂ al., [2019b](https://arxiv.org/html/2408.16202v1#bib.bib155)). Long Short-Term Memory (LSTM)Â (Hochreiter and Schmidhuber, [1997](https://arxiv.org/html/2408.16202v1#bib.bib73)) and Gated Recurrent Units (GRUs)Â (Wang etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib170)) have addressed gradient vanishing, making them more effective for practical applications. These advanced technologies, capable of handling large-scale, high-dimensional, and nonlinear data, provide more accurate and flexible solutions for STELF, making deep learning the preferred technique for STELFÂ (Ahajjam etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib3); Das etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib33); Li etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib107)).

Previous STELF reviews have compiled and examined the research achievements from various perspectives, examining all types of models, or concentrating on certain steps of the forecasting processÂ (Akhtar etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib4); Hou etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib82); AlÂ Mamun etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib5)). Akhtar et al.Â (Akhtar etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib4)), for example, reviewed various STELF models (including time series and regression models), rather than focusing only on Artificial Intelligence (AI) models. Although Hou et al.Â (Hou etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib82)) reviewed load forecasting based on AI models, focusing on data processing and prediction models. Al Mamun et al.Â (AlÂ Mamun etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib5)) reviewed load forecasting, but only focused on hybrid models based on machine learning algorithms. To date, there has been no comprehensive and exhaustive review based on deep learning for STELF that covers the entire forecasting process. This paper aims to fill this gap in the literature.

This article explores the application of deep learning in STELF, providing a comprehensive review of current relevant research. The entire STELF process is examined through a comprehensive review of the literature from 2014 to 2023. The paper is guided by eight research questions (RQs), each of which addresses a key aspect of the STELF process. This survey paper addresses the following key points: (1) a summary and analysis of the literature search results; (2) a classification and description of electricity load datasets; (3) an introduction to STELF data preprocessing methods; (4) an analysis of feature extraction; (5) a description, classification, and summary of STELF models based on deep learning; (6) a review of the optimization process; (7) a summary of evaluation metrics; and (8) a discussion of the challenges and trends for the future of STELF.

The rest of this paper is organized as follows: SectionÂ [2](https://arxiv.org/html/2408.16202v1#S2 "2. Background â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") introduces some background information about the formal description of STELF and the basic deep learning models. SectionÂ [3](https://arxiv.org/html/2408.16202v1#S3 "3. Methodology â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") explains the methodology of this review, including the eight RQs related to STELF, the literature retrieval methods, and the statistical results of the retrieval. SectionsÂ [4](https://arxiv.org/html/2408.16202v1#S4 "4. Answer to RQ1: The Distribution and Analysis of the Search Results â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") to [11](https://arxiv.org/html/2408.16202v1#S11 "11. Answer to RQ8: Challenges and Future Development Trends of STELF â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") answer the eight RQs from SectionÂ [3](https://arxiv.org/html/2408.16202v1#S3 "3. Methodology â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey"), respectively. Finally, SectionÂ [12](https://arxiv.org/html/2408.16202v1#S12 "12. Conclusion â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") concludes the paper.

##  2\. Background 

In this section, we introduce the task definition of STELF and the basic deep learning methods. The primary objective is to quickly familiarize readers with STELF and provide them with an initial understanding of deep learning methods.

###  2.1. STELF Task Definition

STELF aims to predict the electricity load over a future period ranging from a few hours to several days. The modelâ€™s input consists of historical load data and some influencing factors, with the task being to learn a set of mapping functions from input to output. If ytsubscriptğ�‘¦ğ�‘¡y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT represents the load demand at time tğ�‘¡titalic_t, the STELF goal is to predict the load demand within the next hâ„�hitalic_h hours, denoted as y^t+hsubscript^ğ�‘¦ğ�‘¡â„�\hat{y}_{t+h}over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t + italic_h end_POSTSUBSCRIPT. The prediction model can generally be expressed as:

(1) |  | y^t+h=fâ�¢(yt,ytâˆ’1,â€¦,ytâˆ’n+1,Xt),subscript^ğ�‘¦ğ�‘¡â„�ğ�‘“subscriptğ�‘¦ğ�‘¡subscriptğ�‘¦ğ�‘¡1â€¦subscriptğ�‘¦ğ�‘¡ğ�‘›1subscriptğ�‘‹ğ�‘¡\hat{y}_{t+h}=f(y_{t},y_{t-1},\ldots,y_{t-n+1},X_{t}),over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t + italic_h end_POSTSUBSCRIPT = italic_f ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , â€¦ , italic_y start_POSTSUBSCRIPT italic_t - italic_n + 1 end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , |   
---|---|---|---  
  
where y^t+hsubscript^ğ�‘¦ğ�‘¡â„�\hat{y}_{t+h}over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t + italic_h end_POSTSUBSCRIPT is the predicted load at time t+hğ�‘¡â„�t+hitalic_t + italic_h; fğ�‘“fitalic_f is the prediction model (which can be statistical, a machine learning model, or a deep learning model); yt,ytâˆ’1,â€¦,ytâˆ’n+1subscriptğ�‘¦ğ�‘¡subscriptğ�‘¦ğ�‘¡1â€¦subscriptğ�‘¦ğ�‘¡ğ�‘›1y_{t},y_{t-1},\ldots,y_{t-n+1}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , â€¦ , italic_y start_POSTSUBSCRIPT italic_t - italic_n + 1 end_POSTSUBSCRIPT are the historical load data for the previous nğ�‘›nitalic_n time periods; and Xtsubscriptğ�‘‹ğ�‘¡X_{t}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are the external variables at time tğ�‘¡titalic_t (such as weather data, calendar information, etc.).

###  2.2. Basic Deep-Learning Models

In this section, we introduce traditional deep learning models and explore their innovations and variations. We also provide an introduction to the basic definitions and structures of these models, establishing a basis for a more detailed exploration of the application of deep learning methods in STELF.

####  2.2.1. Deep Neural Networks (DNNs)

DNNs are a complex and highly non-linear method for representation learning, typically consisting of an input layer, multiple hidden layers, and an output layerÂ (Din and Marnerides, [2017](https://arxiv.org/html/2408.16202v1#bib.bib36)). Each neuron in the hidden layers functions as a unit that performs mapping within a multi-dimensional data space. Together, these neurons extract complex abstract features and patterns from the input data. Both the width and the depth of DNN networks can be modifiedÂ (Hossen etÂ al., [2017](https://arxiv.org/html/2408.16202v1#bib.bib81)). Shallow neural networks, which have only a single hidden layer, offer only the number of neurons as an adjustable parameter. The strength of DNNs lies not only in their deep structure but also in their non-linear activation functions. Non-linear activation functions, such as ReLU, Sigmoid, or Tanh, enable the network to capture non-linear relationships and complex patterns in the input data. Considering the non-linear nature of ELF load curves (which are influenced by various external factors), the use of DNN as a predictive model is well justifiedÂ (Hossen etÂ al., [2018](https://arxiv.org/html/2408.16202v1#bib.bib80)).

The Deep Belief Network (DBN) is a DNN variant that uses a layered unsupervised learning method for initial weight pre-trainingÂ (Hinton etÂ al., [2006](https://arxiv.org/html/2408.16202v1#bib.bib72)). This layer-by-layer unsupervised training process ensures that each layer effectively captures the features of the preceding layer, allowing the most fundamental features to be extracted from the training set. Typically, a DBN is composed of multiple stacked Restricted Boltzmann Machines (RBMs). An RBM is a type of ANN consisting of a visible layer and a hidden layer, where there are no connections between nodes within the same layer, but nodes between layers are fully connectedÂ (Hafeez etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib64)). Stacked RBMs are used for model pre-training and unsupervised learning, with the top layer fine-tuned using a backpropagation neural network. Fundamentally, an RBM learns a feature representation of a probability distribution over the original input data while also extracting feature informationÂ (Kong etÂ al., [2019](https://arxiv.org/html/2408.16202v1#bib.bib102)).

####  2.2.2. Recurrent Neural Networks (RNNs)

RNNs are particularly effective at processing sequential data, such as time-series dataÂ (GÃ¼rses-Tran etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib63)). The RNN internal loops allow for the continuous transmission of information, which is the use of previous information to influence the current output, a capability also known as the memory functionÂ (Kong etÂ al., [2017](https://arxiv.org/html/2408.16202v1#bib.bib101)).However, RNNs often encounter vanishing or exploding gradient issues when processing long sequences, which hinders their ability to learn long-term dependenciesÂ (Bashir etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib13)). LSTM and GRU were designed to overcome the RNN gradient issues when handling long sequences. They use different memory mechanisms to retain input information over extended periodsÂ (Tayab etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib158); Li etÂ al., [2022a](https://arxiv.org/html/2408.16202v1#bib.bib109)). Faced with the distinct temporal sequences and cyclic patterns in ELF, LSTM, and GRU can utilize historical information for load forecasting and avoid gradient-related issues.

LSTM is a special kind of RNN capable of learning long-term dependencies, specifically designed to address the issue of vanishing gradients. The key to LSTM is its internal structure, the memory cell, which includes four main components: an input gate, a forget gate, an output gate, and a cell state that can maintain information over timeÂ (Haque and Rahman, [2022](https://arxiv.org/html/2408.16202v1#bib.bib67)).

GRU is an improved model based on LSTM, but with a simpler structure and shorter training times, which helps it to better capture long-term dependencies within sequential data. GRU integrates the forget and input gates of LSTM into a single update gate, combining the cell state and hidden state. Compared to LSTM, GRU has fewer parameters, due to having one less gating unit. This significantly improves the computational efficiency.

Both LSTM and GRU improve the information flow control through their gating mechanisms. While LSTM provides precise control mechanisms, GRU improves the computational efficiency of these controls by simplifying them. A choice between these two models typically depends on the specific demands of the task, the nature of the data, and the computational resources available.

####  2.2.3. Convolutional Neural Networks (CNNs)

In recent years, CNNs have become one of the most popular and widely utilized deep-learning modelsÂ (Lu etÂ al., [2019](https://arxiv.org/html/2408.16202v1#bib.bib120)). Although initially designed for processing image data, CNNs are also very effective at handling time-series data. For such data, One-Dimensional (1-D) convolutional layers can capture local patterns and features through a sliding window mechanism. Additionally, CNNs can handle data with grid-like topologies, allowing sequence data to be converted into graph-structured data for processing. A CNN model consists of four main partsÂ (Li etÂ al., [2020a](https://arxiv.org/html/2408.16202v1#bib.bib113); Kim etÂ al., [2019](https://arxiv.org/html/2408.16202v1#bib.bib99)): (i) convolutional layers (which create feature maps from the input data); (ii) pooling layers (which reduce the dimensionality of the convolutional features); (iii) flattening layers (which reshape the data into a column vector); and (iv) fully connected layers (which link the features extracted by the convolutional and pooling layers to other layers).

##  3\. Methodology 

In this section, we present the eight RQs related to STELF that guided our study. This section also provides a detailed introduction to the literature retrieval methods, and the filtering methods used to screen search results. The research methodology of this paper was guided by previous workÂ (Huang etÂ al., [21](https://arxiv.org/html/2408.16202v1#bib.bib91); Zhang etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib201)), and represents a systematic, comprehensive, and rationality approach. For ease of description, we use the term â€œSTELFâ€� to represent the term â€œdeep-learning-based STELFâ€� in this paper, unless explicitly stated.

###  3.1. Research Questions

This paper provides a comprehensive review of the application of deep learning in STELF. It examines the entire STELF process, structured around the following RQs:

  * â€¢

RQ1: What is the distribution and analysis of the literature search results?

  * â€¢

RQ2: What are the electricity load datasets?

  * â€¢

RQ3: How can a dataset be preprocessed for STELF?

  * â€¢

RQ4: What are the methods for feature extraction?

  * â€¢

RQ5: What are the deep-learning-based modeling methods for STELF?

  * â€¢

RQ6: How can the training processes be optimized?

  * â€¢

RQ7: How have the STELF research results been evaluated?

  * â€¢

RQ8: What are the challenges and the future development trends of STELF?




RQ1 explores the distribution of literature related to the use of deep learning for STELF over the past decade, leading to a detailed analysis of this data. RQ2 leads to an overview of electricity load datasets. An answer to RQ3 includes the steps and methods used in data preprocessing. RQ4 leads to a discussion of the deep-learning feature-extraction techniques employed in the prediction process. Answering RQ5 classifies, describes, and analyzes the current state of deep-learning models in STELF. RQ6 explores the methods for optimizing the model-training process. The answer to RQ7 is an organized summary of the evaluation methods used for the forecasting results. Finally, the answer to RQ8 lists the challenges and future development trends of STELF. In the following sections, we provide detailed responses to each RQ, as illustrated in Fig.Â [1](https://arxiv.org/html/2408.16202v1#S3.F1 "Figure 1 â€£ 3.1. Research Questions â€£ 3. Methodology â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey").

![Refer to caption](x1.png)

Figure 1. The structure of this survey paper.

\Description

This figure provides an overview of the survey structure.

###  3.2. Literature Search

The literature search method employed in this paper follows the approach used by Huang et al.Â (Huang etÂ al., [21](https://arxiv.org/html/2408.16202v1#bib.bib91)), involving the following mainstream databases to ensure a comprehensive data collection:

  * â€¢

ACM Digital Library;

  * â€¢

Elsevier Science Direct;

  * â€¢

IEEE Xplore Digital Library;

  * â€¢

Springer Online Library;

  * â€¢

Wiley Online Library;

  * â€¢

MDPI.




The search time range was set to the period 2014 to 2023. An initial attempt using certain keywords for the searchÂ (Zhang etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib201)) in each database revealed that the ELF titles were not uniformly represented. Some papers, for example, did not include words such as â€œelectricityâ€� or â€œpowerâ€� in their titles, even though their actual contents were related to ELF. Furthermore, not all STELF papers had the phrase â€œshort-termâ€� in their titles or keywords list. In addition, because of the diverse terminology used in deep-learning methods, the inclusion of such terminology in the keyword search could result in the omission of relevant papers. To broaden the scope, and avoid missing publications from a particular category, â€œelectricityâ€�, â€œpowerâ€�, â€œshort-termâ€�, and â€œdeep learningâ€� were not included in the keywords. As a result, the final set of keywords used in the search was limited to four phrases: â€œload forecastingâ€�, â€œload forecastâ€�, â€œload predictionâ€�, and â€œload predictingâ€�.

###  3.3. Literature Selection and Statistics

A total of 2,823 papers were retrieved from the six databases, according to the search parameters. These papers were further filtered according to the following selection criteria:

  1. (1)

Not written in English.

  2. (2)

Not discussing ELF.

  3. (3)

Not using deep-learning methods.

  4. (4)

Not focused on â€œshort-termâ€� forecasting research.

  5. (5)

The paper was a review article.




Based on these criteria, 628 papers were selected from the initial 2,823. Additionally, the references of these papers were examined according to the snowballing approachÂ (Huang etÂ al., [21](https://arxiv.org/html/2408.16202v1#bib.bib91)), yielding an additional 22 papers. In total, 650 papers were included in the preliminary review and statistical analysis. The details of this search and filtering are shown in TableÂ [1](https://arxiv.org/html/2408.16202v1#S3.T1 "Table 1 â€£ 3.3. Literature Selection and Statistics â€£ 3. Methodology â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey").

Table 1. Literature search and selection results Digital library |  No. of studies from the search results |  No. of studies after filtering |  No. of studies by snowballing |  Total of filtering and snowballing  
---|---|---|---|---  
ACM Digital Library | 80 | 14 | 0 | 14  
Elsevier Science Direct | 889 | 126 | 0 | 126  
IEEE Xplore Digital Library | 846 | 332 | 22 | 354  
Springer Online Library | 440 | 20 | 0 | 20  
Wiley Online Library | 112 | 25 | 0 | 25  
MDPI | 456 | 111 | 0 | 111  
Total | 2823 | 628 | 22 | 650  
  
We manually processed each of the 650 papers. This process involved an initial checking of all papers, followed by the extraction and recording of key information (including the deep-learning models, datasets, data-preprocessing methods, prediction intervals, model block diagrams, and evaluation metrics). Finally, the content was structured according to Fig.Â [1](https://arxiv.org/html/2408.16202v1#S3.F1 "Figure 1 â€£ 3.1. Research Questions â€£ 3. Methodology â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey"). Although the statistics and analysis results are based on all 650 publications, not all 650 are cited in this paper. Instead, we selectively cited papers with similar content based on the extracted information. Ultimately, this paper thoroughly reviews and cites approximately 200 articles.

##  4\. Answer to RQ1: The Distribution and Analysis of the Search Results 

This section provides the answers to RQ1. Our approach involved an analysis of the publication year trends and their distribution across various literature sources, providing a framework for understanding the evolution and scope of the field.

###  4.1. Publication Trends

We gathered the publication year data of the 650 papers (shown in Fig.Â [2(b)](https://arxiv.org/html/2408.16202v1#S4.F2.sf2 "Figure 2\(b\) â€£ Figure 2 â€£ 4.1. Publication Trends â€£ 4. Answer to RQ1: The Distribution and Analysis of the Search Results â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey")) to show the trends of STELF papers between 2014 and 2023. Fig.Â [2(a)](https://arxiv.org/html/2408.16202v1#S4.F2.sf1 "Figure 2\(a\) â€£ Figure 2 â€£ 4.1. Publication Trends â€£ 4. Answer to RQ1: The Distribution and Analysis of the Search Results â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") shows the number of publications per year, and Fig.Â [2(b)](https://arxiv.org/html/2408.16202v1#S4.F2.sf2 "Figure 2\(b\) â€£ Figure 2 â€£ 4.1. Publication Trends â€£ 4. Answer to RQ1: The Distribution and Analysis of the Search Results â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") shows the cumulative number of publications.

Fig.Â [2(a)](https://arxiv.org/html/2408.16202v1#S4.F2.sf1 "Figure 2\(a\) â€£ Figure 2 â€£ 4.1. Publication Trends â€£ 4. Answer to RQ1: The Distribution and Analysis of the Search Results â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") shows that there were fewer than 10 publications per year during the first three years (2014 to 2016). There has been a rapid growth since 2017, with the number of publications exceeding 100 per year by 2021. Furthermore, an examination of the cumulative numbers of publications (Fig.Â [2(b)](https://arxiv.org/html/2408.16202v1#S4.F2.sf2 "Figure 2\(b\) â€£ Figure 2 â€£ 4.1. Publication Trends â€£ 4. Answer to RQ1: The Distribution and Analysis of the Search Results â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey")) reveals an exponential growth in the research output for STELF. Fig.Â [2(b)](https://arxiv.org/html/2408.16202v1#S4.F2.sf2 "Figure 2\(b\) â€£ Figure 2 â€£ 4.1. Publication Trends â€£ 4. Answer to RQ1: The Distribution and Analysis of the Search Results â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") also shows a linear function with an exceptionally high coefficient of determination (R2=0.9996superscriptğ�‘…20.9996R^{2}=0.9996italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0.9996). The trend shown in Fig.Â [2(a)](https://arxiv.org/html/2408.16202v1#S4.F2.sf1 "Figure 2\(a\) â€£ Figure 2 â€£ 4.1. Publication Trends â€£ 4. Answer to RQ1: The Distribution and Analysis of the Search Results â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") is directly related to the rapid development of deep-learning technologies in recent years. It also highlights the significance of research in this field.

![Refer to caption](x2.png) (a) Number of publications per year.

![Refer to caption](x3.png) (b) Cumulative number of publications per year.

Figure 2. STELF papers published between January 1, 2014, and December 31, 2023.

\Description

This figure depicts the distribution of publication years and the cumulative distribution over the years.

###  4.2. Types of Publication Venues

The papers were sourced from multiple journals and conferences, with their proportion and distribution plotted in Fig.Â [3(b)](https://arxiv.org/html/2408.16202v1#S4.F3.sf2 "Figure 3\(b\) â€£ Figure 3 â€£ 4.2. Types of Publication Venues â€£ 4. Answer to RQ1: The Distribution and Analysis of the Search Results â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey"). Fig.Â [3(a)](https://arxiv.org/html/2408.16202v1#S4.F3.sf1 "Figure 3\(a\) â€£ Figure 3 â€£ 4.2. Types of Publication Venues â€£ 4. Answer to RQ1: The Distribution and Analysis of the Search Results â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") shows that the number of publications published in journals (55%) exceeds those in conferences (45%). Fig.Â [3(b)](https://arxiv.org/html/2408.16202v1#S4.F3.sf2 "Figure 3\(b\) â€£ Figure 3 â€£ 4.2. Types of Publication Venues â€£ 4. Answer to RQ1: The Distribution and Analysis of the Search Results â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") shows that, apart from 2017, the number of journal publications consistently outpaces the number of conference publications. Fig.Â [3(b)](https://arxiv.org/html/2408.16202v1#S4.F3.sf2 "Figure 3\(b\) â€£ Figure 3 â€£ 4.2. Types of Publication Venues â€£ 4. Answer to RQ1: The Distribution and Analysis of the Search Results â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") also shows a trend, for both journals and conferences, of increasing publication volume.

![Refer to caption](x4.png) (a) Proportion of journals and conferences.

![Refer to caption](x5.png) (b) Venue distribution per year.

Figure 3. Venue distribution of surveyed papers.

\Description

This figure provides a description of the classification and distribution for publication.

##  5\. Answer to RQ2: The Electricity Load Datasets 

This section provides the answers to RQ2, which discusses the classification of datasets and examines some common public datasets. When selecting electricity load datasets, researchers need to choose the appropriate type of dataset based on the specific requirements of the forecasting scenario, which may include loads for residential households, commercial buildings, industry, and entire citiesâ€™ system-level load.

Electricity load datasets play a crucial role in STELF. ELF typically relies on historical data to predict future electricity demand over specific periods. With the advancement of deep learning methods, these models require substantial amounts of data to train for accurate predictions. Electricity load datasets provide comprehensive historical electricity usage information, including load changes during different periods, consumer usage patterns, and the impact of seasonal and weather factors on electricity demandÂ (Zhang etÂ al., [2022c](https://arxiv.org/html/2408.16202v1#bib.bib203)). This information is vital for a deep understanding and accurate prediction of electricity demand patterns.

###  5.1. Classification of Datasets

Electricity load datasets can be broadly categorized based on their accessibility as either public or not. Public datasets are typically available online, and are usually provided by government agencies, power market operators, or research institutionsÂ (Giacomazzi etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib59); Li etÂ al., [2020b](https://arxiv.org/html/2408.16202v1#bib.bib111); Xia etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib179); Khan etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib98); Atef and Eltawil, [2020](https://arxiv.org/html/2408.16202v1#bib.bib11)). Non-public datasets, in contrast, are usually not available due to considerations such as protecting competitive advantage, ensuring national security, complying with legal regulations, or protecting personal privacy.

###  5.2. Common Public Datasets

The review of the 650 papers revealed a wide variety of datasets. Some papers (such asÂ (Deepanraj etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib35); Sun etÂ al., [2018](https://arxiv.org/html/2408.16202v1#bib.bib153); Chen etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib27))) did not mention the name or source of the dataset, while others (such asÂ (Wang etÂ al., [2020a](https://arxiv.org/html/2408.16202v1#bib.bib172); Arastehfar etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib9); Zhang etÂ al., [2023c](https://arxiv.org/html/2408.16202v1#bib.bib207))) used multiple datasets, making it difficult to compile statistics. Table [2](https://arxiv.org/html/2408.16202v1#S5.T2 "Table 2 â€£ 5.2. Common Public Datasets â€£ 5. Answer to RQ2: The Electricity Load Datasets â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") lists some of the most frequently used public datasets, which are:

  * â€¢

Independent System Operator of New England (ISO-NE)111<https://www.iso-ne.com/>.: The dataset includes hourly electrical load, temperature, day type, and other information for the New England area of North America, from March 2003 to December 2014.

  * â€¢

Australian Energy Market Operator (AEMO)222<https://www.aemo.com.au/energy-systems/electricity/national-electricity-market-nem/data-nem/aggregated-data>.: The dataset contains 30-minute time-series data on electrical loads for five regions in Australia (South Australia, Queensland, New South Wales, Western Australia, and Victoria).

  * â€¢

Global Energy Forecasting Competition (GEFCom)333<https://www.kaggle.com/c/global-energy-forecasting-competition-2012-load-forecasting/data>.: Each competition provides different datasets that cover historical data and related influencing factors for various regions and periods. The design of the datasets reflects the real-world conditions of energy markets and system operations.

  * â€¢

University of California, Irvine (UCI) Machine Learning Repository (MLR)444<https://archive.ics.uci.edu/ml/datasets>.: This dataset is a widely used public database covering simple datasets to complex multivariate time-series datasets. The household load data uses a sampling frequency of one minute.

  * â€¢

European Network of Transmission System Operators for Electricity (ENTSO-E)555<https://open-power-system-data.org/data-sources##1_European_load_data>.: This dataset is from an organization of electricity transmission system operators covering 35 European countries. The dataset includes real-world hourly electricity load time series from across Europe.

  * â€¢

PJM Interconnection (PJM)666<https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption>.: This dataset is from a regional transmission organization in the United States responsible for operating the Eastern Interconnection grid. PJM transmits electricity to 14 regions in the United States, with data recorded at hourly MW intervals.

  * â€¢

Commission for Energy Regulation (CER)777<https://www.cru.ie/>.: This dataset records the half-hourly load data of residential households and small to medium-sized enterprises in Ireland.

  * â€¢

2016 China Electrical Mathematical Modeling Competition888<https://github.com/huberyCC/Load-datasets>.: This dataset originates from the China Electrical Engineering Mathematical Modeling Competition and includes electrical load data and weather data from 2009 to 2015.




All the datasets listed in TableÂ [2](https://arxiv.org/html/2408.16202v1#S5.T2 "Table 2 â€£ 5.2. Common Public Datasets â€£ 5. Answer to RQ2: The Electricity Load Datasets â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") have been used at least 10 times. The frequent use of ISO-NE and AEMO especially, both of which exceed 30 times, highlights their significant role and the high level of activity these datasets sustain in ELF research. The sampling frequency of the datasets varies from every minute to every hour, highlighting the diversity of real-time granularity, which supports a wide range of research and practical applications. Many important power datasets are covered, and distributed across multiple regions (including North America, Australia, Europe, and China).

The majority of the public datasets can be accessed online through platforms such as Kaggle, official websites, and specialized data repositories. This facilitates their access and use by researchers worldwide. Open access and faster updates are two major trends in the development of power datasets.

Table 2. Commonly Used Electricity Datasets Dataset | Usage Frequency | Sampling Frequency  
---|---|---  
Independent System Operator of New England (ISO-NE) | 47 | 1 hour  
Australian Energy Market Operator (AEMO) | 35 | 30 minutes  
Global Energy Forecasting Competition (GEFCom) | 23 | 1 hour  
UCI Machine Learning Repository (UCI) | 23 | 1 minute  
European Network of Transmission System Operators for Electricity (ENTSO-E) | 15 | 1 hour  
PJM Interconnection (PJM) | 11 | 1 hour  
Commission for Energy Regulation (CER) | 10 | 30 minutes  
2016 China Electrical Mathematical Modeling Competition | 10 | 15 minutes  
  
##  6\. Answer to RQ3: STELF Dataset Preprocessing 

In this section, we address RQ3, introducing the main data-preprocessing methods (including data cleaning, selection of external variables, and data reconstruction).

Data preprocessing is a crucial step that directly impacts the accuracy and reliability of the prediction models. Raw electricity-load data frequently contains noise, anomalies, and inconsistent records. There are also often missing values. Unaddressed, these issues can significantly disrupt the learning process of models, leading to inaccurate or ineffective predictionsÂ (Meng etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib125)). Data preprocessing uses a series of methods (such as filling in missing values, and anomaly detection, correction, and normalization) to enhance the data qualityÂ (Subbiah and Chinnappan, [2022](https://arxiv.org/html/2408.16202v1#bib.bib151)). The data also often exhibits strong temporal characteristics and seasonal variability. Appropriate preprocessing can help models capture these complex patterns, enhancing their understanding and responsiveness to temporal dynamicsÂ (Lv etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib123)). This section examines the various data-preprocessing steps and methods. It should be noted that published papers generally only mention one or several data preprocessing steps, not all. For example, Dong et al.Â (Dong etÂ al., [2021a](https://arxiv.org/html/2408.16202v1#bib.bib40)) only discussed data normalization; Gao et al.Â (Gao etÂ al., [2023a](https://arxiv.org/html/2408.16202v1#bib.bib56)) focused only on handling missing data; and Huang et al.Â (Dogra etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib37)) introduced data standardization and data reconstruction. Specific preprocessing measures are usually applied based on the design requirements of the model.

###  6.1. Data Cleaning

The data-cleaning process involves handling missing values, outliers, erroneous records, duplicate data, and performing standardization.

Various data-imputation techniques can be used to fill in the gaps of missing dataÂ (Rafati etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib137); Gan etÂ al., [2017](https://arxiv.org/html/2408.16202v1#bib.bib54)). Some simple approaches include using the value from a previous time point or calculating the average of data from before and after the missing value (Li etÂ al., [2019](https://arxiv.org/html/2408.16202v1#bib.bib108); Chen etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib25); Hua etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib87)). Other methods include data clusteringÂ (Li etÂ al., [2022b](https://arxiv.org/html/2408.16202v1#bib.bib112)), and using values from the same time on adjacent datesÂ (Zhang etÂ al., [2022a](https://arxiv.org/html/2408.16202v1#bib.bib199)).

Outlier detection is often addressed using the three-sigma methodÂ (Khan etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib98); Chandola etÂ al., [2009](https://arxiv.org/html/2408.16202v1#bib.bib21)). Sharma et al.Â (Sharma and Jain, [2022](https://arxiv.org/html/2408.16202v1#bib.bib147)) also used the interquartile range for this purpose, while Qin et al.Â (Qin etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib136)) used box plots. Typically, when encountering outliers, erroneous records, or duplicated data, the main remedial strategies involve deletion or replacement (using established methods for handling missing values).

Data standardization relates to eliminating scale differences in the original data, allowing for comparison and calculation on the same scale. The use of raw data for analysis may lead to biases towards features with larger numerical rangesÂ (Tan etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib154)). Two methods for data standardization are Min-Max Scaling and Z-Score NormalizationÂ (Wang etÂ al., [2023b](https://arxiv.org/html/2408.16202v1#bib.bib174); Huang etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib88)).

Min-Max Scaling adjusts the data to fit within a specified range, commonly between 0 and 1. The formula for Min-Max Scaling is:

(2) |  | xnorm=xâˆ’minâ�¡(x)maxâ�¡(x)âˆ’minâ�¡(x),subscriptğ�‘¥normğ�‘¥ğ�‘¥ğ�‘¥ğ�‘¥x_{\text{norm}}=\frac{x-\min(x)}{\max(x)-\min(x)},italic_x start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT = divide start_ARG italic_x - roman_min ( italic_x ) end_ARG start_ARG roman_max ( italic_x ) - roman_min ( italic_x ) end_ARG , |   
---|---|---|---  
  
where xğ�‘¥xitalic_x is the original data value; minâ�¡(x)ğ�‘¥\min(x)roman_min ( italic_x ) and maxâ�¡(x)ğ�‘¥\max(x)roman_max ( italic_x ) are the minimum and maximum values of the original data, respectively; and xnormsubscriptğ�‘¥normx_{\text{norm}}italic_x start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT is the scaled data.

Z-Score Normalization shifts the dataâ€™s mean to 0 and standard deviation to 1, making it suitable for data that requires outlier mitigation and handling of skewed distributions. The formula for Z-Score Standardization is:

(3) |  | z=(xâˆ’Î¼)Ïƒ,ğ�‘§ğ�‘¥ğ�œ‡ğ�œ�z=\frac{(x-\mu)}{\sigma},italic_z = divide start_ARG ( italic_x - italic_Î¼ ) end_ARG start_ARG italic_Ïƒ end_ARG , |   
---|---|---|---  
  
where xğ�‘¥xitalic_x is the original data value; Î¼ğ�œ‡\muitalic_Î¼ is the mean of the data; and Ïƒğ�œ�\sigmaitalic_Ïƒ is the standard deviation.

###  6.2. External Variable Selection

The accuracy of STELF is determined not only by the operational conditions within the power system, but also by carefully considering a series of important external variablesÂ (Wang etÂ al., [2023d](https://arxiv.org/html/2408.16202v1#bib.bib169)). The next task after completing the data cleaning is to identify the external variables that significantly impact the forecasting resultsÂ (Cai etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib20)), such as the variability of climatic conditions, periodic fluctuations in temperature, holiday status, and dynamic changes in industrial activities. Appropriate consideration of these external factors can lead to a more comprehensive load forecasting model. This usually involves correlation analysis and variable-importance evaluation.

TableÂ [3](https://arxiv.org/html/2408.16202v1#S6.T3 "Table 3 â€£ 6.2. External Variable Selection â€£ 6. Answer to RQ3: STELF Dataset Preprocessing â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") lists six commonly used methods for external variable selection, and some of the papers that use them. The Pearson Correlation Coefficient, for example, calculates the Pearson correlation between two continuous variables, with values ranging from -1 to 1. By determining a threshold for the correlation coefficient, only variables whose correlation with the target variable exceeds this threshold are considered to be correlated.

Zheng et al.Â (Zheng etÂ al., [2018](https://arxiv.org/html/2408.16202v1#bib.bib208)) used the Least Absolute Shrinkage and Selection Operator (LASSO) method to perform variable selection. Subbiah et al.Â (Subbiah and Chinnappan, [2022](https://arxiv.org/html/2408.16202v1#bib.bib151)) introduced the Robust ReliefF Mutual Information Recursive Feature Elimination Hybrid Feature Selection (RMR-HFS), which uses a combination of filter and wrapper methods for variable selection. These selection methods cover a wide range of data analysis needs, from linear to nonlinear correlations, from time series analysis to dimensionality-reduction techniques. Each method has its unique advantages, helping to better understand and uncover correlations within the data, and enabling the construction of more accurate and efficient predictive models.

Table 3. Common Methods for Selecting External Variables Selection Methods |  Example reference  
---|---  
Pearson Correlation Coefficient |  (Wang etÂ al., [2020a](https://arxiv.org/html/2408.16202v1#bib.bib172); Tang etÂ al., [2019a](https://arxiv.org/html/2408.16202v1#bib.bib157); Shaqour etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib146); Bian etÂ al., [2022a](https://arxiv.org/html/2408.16202v1#bib.bib16); Hossain and Mahmood, [2020](https://arxiv.org/html/2408.16202v1#bib.bib79); Dong etÂ al., [2017](https://arxiv.org/html/2408.16202v1#bib.bib39); Xie etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib181))  
Copula Function Theory |  (Wang etÂ al., [2023d](https://arxiv.org/html/2408.16202v1#bib.bib169); He etÂ al., [2017](https://arxiv.org/html/2408.16202v1#bib.bib71))  
Maximum Information Coefficient |  (Tang etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib156); Xiang etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib180); Jiao etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib96); Lu etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib121); Huang etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib89))  
Autocorrelation Function |  (Li etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib110); Javed etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib94); Farid etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib49))  
Spearmanâ€™s Rank Correlation Analysis |  (Hong and Chan, [2023](https://arxiv.org/html/2408.16202v1#bib.bib75); Liu etÂ al., [2022c](https://arxiv.org/html/2408.16202v1#bib.bib117); Hu etÂ al., [2022c](https://arxiv.org/html/2408.16202v1#bib.bib85); Zamee etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib195))  
Principal Component Analysis |  (Veeramsetty etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib162); Han etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib66))  
  
###  6.3. Data Reconstruction

Data reconstruction plays a key role in dealing with the randomness, volatility, periodicity, and diversity (Kim etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib100)) of raw load data, and has become one of the key focus points in many STELF studies. A deep exploration of historical load data combined with advanced analytical methods (such as decomposition and clustering techniques) can reveal the key recurring patterns and trends in the data. These things are crucial for predictive models, as they help the models better understand and capture the dependencies within time-series data.

Several common methods are used for data reconstruction. The Variational Mode Decomposition (VMD) technique decomposes load data into a series of Intrinsic Mode Functions (IMFs), which are then used to reconstruct the data for trainingÂ (Ahajjam etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib3)). Zang et al.Â (Zang etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib196)) also used VMD technology to decompose the load data into modalities of different frequencies, employing LSTM with self-attention mechanism for forecasting. This multi-frequency analysis method allows for a more nuanced handling of the complexity within load data. Similarly, Mathew et al.Â (Mathew etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib124)) used Empirical Mode Decomposition (EMD) to decompose the raw data into a series of IMFs, and then used the same model to train each mode. Based on EMD, Ensemble Empirical Mode Decomposition (EEMD)Â (Yue etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib194)) and Improved Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (ICEEMDAN)Â (Zhang etÂ al., [2023f](https://arxiv.org/html/2408.16202v1#bib.bib206)) are also used for data reconstruction.

Advanced clustering methods have also been used for data reconstruction (Wu etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib178); Yang etÂ al., [2019](https://arxiv.org/html/2408.16202v1#bib.bib186); Wang etÂ al., [2020b](https://arxiv.org/html/2408.16202v1#bib.bib168)). Wu et al.Â (Wu etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib178)) applied K-shape time-series clustering to categorize users with similar electricity usage habits and characteristics into multiple types. Yang et al.Â (Yang etÂ al., [2019](https://arxiv.org/html/2408.16202v1#bib.bib186)) used the K-means clustering algorithm to identify customer groups with similar electricity usage behaviors. Wang et al.Â (Wang etÂ al., [2020b](https://arxiv.org/html/2408.16202v1#bib.bib168)) employed the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm to deal with datasets that contained noise. Chaturvedi et al.Â (Chaturvedi etÂ al., [2015](https://arxiv.org/html/2408.16202v1#bib.bib23)) used wavelet transform technology to decompose load data into four wavelet components and then trained a neural network for each component, achieving precise predictions of different frequency characteristics.

TableÂ [4](https://arxiv.org/html/2408.16202v1#S6.T4 "Table 4 â€£ 6.3. Data Reconstruction â€£ 6. Answer to RQ3: STELF Dataset Preprocessing â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") summarizes some common data-reconstruction methods. Whether using decomposition or clustering techniques, the goal is to reconstruct the overall data to capture the distribution characteristics and underlying patterns of the data. Due to the complexity of load data, adopting a divide-and-conquer approach (where each part is trained using the same or different models) can enhance the efficiency and accuracy of the model. Reconstruction techniques not only provide a solid data foundation for the models but also directly influence the modelâ€™s design, the algorithm selection, and the precision of the forecasting outcomes.

Table 4. Summary of Common Data Reconstruction Methods Reconstruction method | Abbreviation |  Example reference |  Description  
---|---|---|---  
Variational Mode Decomposition | VMD |  (Wu etÂ al., [2023b](https://arxiv.org/html/2408.16202v1#bib.bib176); Zang etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib196); Zhuang etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib210)) |  A decomposition method based on the variational principle, solving intrinsic mode functions through optimization problems.  
Empirical Mode Decomposition | EMD |  (Fan etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib48); Mounir etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib127); Ran etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib138)) |  An empirical decomposition method that extracts intrinsic mode functions through an iterative process.  
Wavelet Transform | WT |  (Chaturvedi etÂ al., [2015](https://arxiv.org/html/2408.16202v1#bib.bib23); Zhang etÂ al., [2022c](https://arxiv.org/html/2408.16202v1#bib.bib203)) |  A method that uses a set of wavelet functions to analyze signals, providing both time (or spatial) and frequency information about the signals.  
Density-Based Spatial Clustering of Applications with Noise | DBSCAN |  (Yang etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib185); Wang etÂ al., [2020b](https://arxiv.org/html/2408.16202v1#bib.bib168); Kong etÂ al., [2017](https://arxiv.org/html/2408.16202v1#bib.bib101)) |  A density-based clustering algorithm that classifies points as cluster members, noise, or border points based on their density.  
K-means Clustering Algorithm | K-means |  (Hu etÂ al., [2022a](https://arxiv.org/html/2408.16202v1#bib.bib86); Yang etÂ al., [2019](https://arxiv.org/html/2408.16202v1#bib.bib186)) |  A clustering algorithm that partitions a dataset into K distinct, non-overlapping groups based on the similarity of data points.  
K-shape Clustering Algorithm | K-shape |  (Wu etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib178); Fahiman etÂ al., [2017](https://arxiv.org/html/2408.16202v1#bib.bib47)) |  A clustering algorithm for time-series data that partitions the data into different groups based on shape similarity.  
Singular Spectrum Analysis | SSA |  (Niu etÂ al., [2016](https://arxiv.org/html/2408.16202v1#bib.bib132); Nie etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib131)) |  A method for decomposing time-series data by extracting trend, periodic, and noise components to analyze the time series.  
  
##  7\. Answer to RQ4: Methods for Feature Extraction 

This section provides the answers to RQ4, examining how deep learning can effectively extract deep non-linear features from the data, enhancing the modelâ€™s predictive capabilities. The purpose of feature extraction is to mine complex relationships from the raw load data that aid the model in understanding load changes. In STELF, feature extraction is a critical step, as it directly impacts the performance of the predictive model.

Deep learning can extract features in an unsupervised learning manner, automatically learning useful feature representations from the dataÂ (Wang etÂ al., [2019b](https://arxiv.org/html/2408.16202v1#bib.bib165)). At different levels of the ANN, features at various levels of abstraction can be learned, providing the model with rich information. In STELF, in addition to considering the feature relationships within the time series itself, spatial feature relationships must also be considered (Cheung etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib29)), such as for large-scale ELF involving regional or urban power grids with distinct spatial relationships. The spatial characteristics are also crucial for fully understanding load change patterns and enhancing the accuracy of forecasts.

###  7.1. Temporal Feature Relationship Extraction

Because electricity-load data are time-series data, RNNs can be used to capture the temporal dependencies. LSTMs and GRUs are often used in temporal feature relationship extraction. Xu et al.Â (Xu etÂ al., [2018](https://arxiv.org/html/2408.16202v1#bib.bib183)) used an LSTM to extract deep features of electricity loads and employed an Extreme Learning Machine (ELM) to model shallow patterns. Abdel et al.Â (Abdel-Basset etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib2)) proposed STLF-Net, using a GRU to get the long-term temporal representations of data.

The success of feature extraction models based on the Multi-Channel One-Dimensional Convolutional Neural Network (MCNN) (Dong etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib38)) suggests that convolution operations can also be used to extract feature relationships in time-series data. This allows for the direct capture of inter-feature relationships on sequence data through One-Dimensional Convolution (Conv1D) operations. Although One-Dimensional CNNs (1-D CNNs) are functionally similar to RNNs (such as LSTM and GRU), they also have unique network structural designs to accommodate the varying characteristics and requirements of time-series data. These structural designs make it possible for 1-D CNNs to be optimized for specific types of sequence data, making them better at extracting useful feature relationships. Dai et al.Â (Dai etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib32)) used CNNs with Conv1D and pooling layers, where the Conv1D layer extracts pivotal features from the input data, and a pooling layer reduces the dimensionality and spatial complexity of the features.

Temporal Convolutional Networks (TCNs) are another type of ANN designed for sequence modeling tasks, especially those involving time-series data. Because the TCN is based on CNN (Bai etÂ al., [2018](https://arxiv.org/html/2408.16202v1#bib.bib12)), it has also been widely used to extract feature vectors and long-term temporal dependencies (Bian etÂ al., [2022a](https://arxiv.org/html/2408.16202v1#bib.bib16); Wang etÂ al., [2020a](https://arxiv.org/html/2408.16202v1#bib.bib172); Zhang etÂ al., [2023c](https://arxiv.org/html/2408.16202v1#bib.bib207)). Zhang et al.Â (Zhang etÂ al., [2023c](https://arxiv.org/html/2408.16202v1#bib.bib207)) proposed a hybrid network architecture combining TCN and LSTM to address the issue of model complexity. Their model leveraged the TCNâ€™s ability to capture the receptive field in time series and effectively model temporal dependencies, while also incorporating the LSTMâ€™s ability to handle long-term dependency problems.

###  7.2. Spatial Feature Relationship Extraction

Extraction of spatial feature relationships involves the construction of an adjacency matrix to represent the connections between nodes in the power network. Multi-dimensional convolution is used to obtain the spatial features. Hua et al.Â (Hua etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib87)) proposed a predictive model that combines CNN and GRU, extracting spatial features through the CNN and temporal features through the GRU. Wan et al.Â (Wan etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib164)) used CNNs to extract spatial information from the load data, with the resulting features then input into the RNN for training.

Another approach involves the construction of a spatio-temporal graph model that can simultaneously capture the spatial and temporal dependencies of electricity-load data. Yu et al.Â (Yu and Li, [2021](https://arxiv.org/html/2408.16202v1#bib.bib192)) fed graph-structured data into a Spatio-Temporal Synchronous Graph Convolutional Network (STSGCN) model to perform load forecasting by extracting the inherent spatio-temporal features from historical load data. The spatio-temporal graph model constructs a similarity-weighted spatio-temporal graph by combining the feature sets of multiple nodesÂ (Huang etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib89)). The Spatial Convolutional Layer (SCL) extracts the features of neighboring nodes for each node in the graph, thereby enhancing the full-domain node features.

Features extracted using deep learning cannot be directly applied to STELF. A supervised learning-regression process is required to transform these nonlinear features into prediction results. A variety of methods (such as linear/nonlinear regression, or neural networks) can be used to perform this mapping (Wang etÂ al., [2019b](https://arxiv.org/html/2408.16202v1#bib.bib165)).

##  8\. Answer to RQ5: Deep-Learning-Based Modeling Methods for STELF 

This section provides the answers to RQ5, offering an extensive review of the literature on deep-learning-based predictive models. From the perspective of forecasting outcomes, predictive models can be categorized as either deterministic or probabilistic (Benidis etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib15)).

###  8.1. Deterministic Forecasting Models

In STELF, deterministic forecasting provides an exact numerical prediction, offering precise predictions of the load level at a specific point in time or over a certain period in the future. This type of forecasting focuses on delivering a concrete value rather than a range or distribution. Due to the extensive literature and methods involved, we examine this type of forecasting from the perspectives of single and hybrid models.

####  8.1.1. Single Models

Simplicity is the main advantage of the single model, which is easy to understand and construct. Deep learning algorithms make use of deep networks, consisting of a series of complex hidden layersÂ (Eren and KÃ¼Ã§Ã¼kdemiral, [2024](https://arxiv.org/html/2408.16202v1#bib.bib44)). Early STELF achieved predictive results by stacking ANNsÂ (Singh and Dwivedi, [2018](https://arxiv.org/html/2408.16202v1#bib.bib149); Din and Marnerides, [2017](https://arxiv.org/html/2408.16202v1#bib.bib36)). Chen et al.Â (Chen etÂ al., [2019](https://arxiv.org/html/2408.16202v1#bib.bib24)) and Hossen et al.Â (Hossen etÂ al., [2017](https://arxiv.org/html/2408.16202v1#bib.bib81)) explored ELF using DNNs. Chen et al. proposed a method based on two-terminal sparse coding and deep neural network fusion, while Hossen et al. examined the impact of single-layer versus double-layer DNN architectures. A DBN, similar in structure to DNN, combines multiple RBMs for ELF and uses a layer-by-layer unsupervised learning method to pre-train the initial weightsÂ (Dedinec etÂ al., [2016](https://arxiv.org/html/2408.16202v1#bib.bib34)).

DNNs or DBNs formed by stacking multiple layers typically lack memory capability, which means that they may not be able to use previous information effectively when processing time-series data. RNNs use recurrent connections, allowing the network to retain previous information while processing sequence data, potentially adjusting the output based on earlier elements in the sequence. This makes RNNs (including LSTM and GRU, as introduced in SectionÂ [2](https://arxiv.org/html/2408.16202v1#S2 "2. Background â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey")) very popular for time-series forecasting. LSTMs and GRUs are advanced RNNs that have been used in STELFÂ (Kong etÂ al., [2017](https://arxiv.org/html/2408.16202v1#bib.bib101); Morais etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib126)). Zhu et al.Â (Zhu etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib209)) proposed a dual-attention encoder-decoder structure using attention mechanisms, using an LSTM as a specific encoder and decoder for nonlinear dynamic time modeling. This attention mechanism dynamically focused on the importance of different parts of the input sequence, significantly enhancing the performance of RNNs used in conjunction. Aseeri et al.Â (Aseeri, [2023](https://arxiv.org/html/2408.16202v1#bib.bib10)) also used a GRU structure to focus on key variables, improving performance, particularly with longer sequences.

A bidirectional RNN is an innovative ANN architecture that integrates two recurrent layers, each with a distinct role. One layer captures the forward flow of the sequence, while the other focuses on the backward flow. This bidirectional processing mechanism makes it possible to comprehensively understand the sequence data, enabling feature and information extraction from both temporal directions simultaneously (Mughees etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib128)). This bidirectional structure, such as in the Bidirectional Long Short-Term Memory (BiLSTM)Â (Wang etÂ al., [2019c](https://arxiv.org/html/2408.16202v1#bib.bib171)) and Bidirectional Gated Recurrent Unit (BiGRU)Â (Shaqour etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib146)), has a strong predictive capability, and is popular in STELF research.

Using a CNN as a standalone STELF model involves treating time-series data directly as one-dimensional images or creating images from the sequence values of multivariate time seriesÂ (Sadaei etÂ al., [2019](https://arxiv.org/html/2408.16202v1#bib.bib141)). The convolutional layers in CNNs are used to extract local features; while pooling layers reduce the dimensionality of time-series data â€” this helps with both extracting more abstract features and reducing computational complexityÂ (Jalali etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib92)). Finally, one or more fully connected layers are used to generate the output sequence. As a variant of CNNs, TCNs extend the receptive field and capture long-range sequence dependencies by increasing the spacing of convolutional kernels (Bai etÂ al., [2018](https://arxiv.org/html/2408.16202v1#bib.bib12)). TCNs extract the complex interactions between time-series and non-time-series data, resulting in precise feature quantitiesÂ (Bian etÂ al., [2022b](https://arxiv.org/html/2408.16202v1#bib.bib17)). Yin et al.Â (Yin and Xie, [2021](https://arxiv.org/html/2408.16202v1#bib.bib190)) used TCNs to extract key features from multiple spatial scale samples, yielding initial predictive outcomes. A similar model based on CNN, WaveNet, is a generative model developed by DeepMind, primarily used for generating audio waveformsÂ (Van DenÂ Oord etÂ al., [2016](https://arxiv.org/html/2408.16202v1#bib.bib159)). WaveNet has also been used for STELFÂ (VoÃŸ etÂ al., [2018](https://arxiv.org/html/2408.16202v1#bib.bib163); Lin etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib115)).

The Transformer model, proposed by Vaswani et al.Â (Vaswani etÂ al., [2017](https://arxiv.org/html/2408.16202v1#bib.bib161)) in 2017, has seen great success in various natural language processing tasks, and has also been extended to other fields such as image processing and time-series forecasting. The Transformerâ€™s powerful feature-extraction and sequence-modeling capabilities also make it a good choice for STELFÂ (Zhang etÂ al., [2023b](https://arxiv.org/html/2408.16202v1#bib.bib202), [2022b](https://arxiv.org/html/2408.16202v1#bib.bib198); Nawar etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib129)).

Two models based on the Transformer are the Informer and the Temporal Fusion Transformer (TFT)Â (Gao etÂ al., [2023b](https://arxiv.org/html/2408.16202v1#bib.bib57); Santos etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib143)). The Informer model focuses on the most critical time steps, using a probabilistic sparsity approach, ignoring less important information. Gao et al.Â (Gao etÂ al., [2023b](https://arxiv.org/html/2408.16202v1#bib.bib57)), for example, established a hybrid ELF model based on the Informer. Yu et al.Â (Yu etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib191)) proposed a self-attention-based STELF method considering demand-side management, using an informer to independently predict and reconstruct the decomposed intrinsic mode function components. The TFT model focuses on how to integrate different types of time-series data to improve prediction accuracy. Giacomazzi et al.Â (Giacomazzi etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib59)) explored the potential of TFT for hourly STELF across different time ranges (such as the previous day and the previous week).

####  8.1.2. Hybrid Models

In STELF, hybrid deep-learning models are becoming a key technology for solving complex forecasting problems. Hybrid models integrate a variety of deep-learning models, with advantages including their diversity and flexibility. Complementing each otherâ€™s strengths, they enhance the accuracy and robustness of predictionsÂ (Lin etÂ al., [2022b](https://arxiv.org/html/2408.16202v1#bib.bib116)). The hybrid model primarily employs two strategies for combination: stage-wise training and joint training. Stage-wise training involves focusing on specific learning tasks at each stage, while joint training involves training all components of the hybrid model simultaneously.

(1) Stage-wise Training: Stage-wise training strategy addresses some challenges for training complex models by breaking down the training process into a series of orderly stages. In each stage, a part of the model is independently trained and optimized to learn specific patterns.

A hybrid model integrating CNN with RNN has become a widely adopted solution, due to its exceptional performanceÂ (Feng etÂ al., [2024](https://arxiv.org/html/2408.16202v1#bib.bib53); Shi etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib148); Yi etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib189); Sekhar and Dahiya, [2023](https://arxiv.org/html/2408.16202v1#bib.bib145); Aouad etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib8); Guo etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib61)). Zhang et al.Â (Zhang etÂ al., [2023a](https://arxiv.org/html/2408.16202v1#bib.bib205)) proposed a hybrid model based on CNN and LSTM, using CNN layers for feature extraction from the input dataset and an LSTM model for sequence prediction, supporting multi-step forecasting of time-series data. Their approach leverages the efficient CNN capability to extract local features, using their output feature vectors as inputs for the RNN, which allows the RNN to further capture the dynamic changes and long-term dependencies in the time series. Jin et al.Â (Jin etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib97)) used a CNN-GRU hybrid model based on parameter-transfer learning. By transferring the parameters of a trained model from one with a large dataset to another trained with a smaller dataset, the modelâ€™s performance and predictive accuracy were enhanced. Here, transfer learning was used to address the issue of insufficient training dataÂ (Ozer etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib133)).

There are also some novel hybrid models based on the stage-wise structure. The combination of Graph Neural Network (GNN) and TCN passes the features extracted by the GNN to the TCN for further trainingÂ (Lin etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib115)). The combination of LSTM and TFT uses the LSTM as an encoder-decoder to preprocess the data, which is then fed as training data into the TFTÂ (Santos etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib143)). A more complex fusion proposed by Bu et al.Â (Bu etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib18)) involves a hybrid model combining Conditional Generative Adversarial Networks (CGAN) with CNN. The CGAN uses the CNNâ€™s ability to accurately capture internal features to generate realistic fake samples, while the semi-supervised regression layer optimizes the discriminator to enhance sample authenticity recognition.

(2) Joint Training: Joint training integrates various components of a model into a unified training framework, enabling synchronous training of each component. Data reconstruction techniques can result in different modalities, requiring models to be trained for each modality (Wang etÂ al., [2023d](https://arxiv.org/html/2408.16202v1#bib.bib169); Zhang etÂ al., [2023d](https://arxiv.org/html/2408.16202v1#bib.bib200); Luo etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib122)). This approach leverages the characteristics of the different modalities, training concurrently, enhancing the overall performance.

Data can be divided into multiple components, with different models being used to train separately and simultaneously on their respective data. He et al.Â (He etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib69)) used Per-unit Curve Rotation Decoupling (PCRD) to decompose the load into three parts: the rotating unit-load curve; the zero AM load; and the daily average load. A CNN extracted the shape features of the rotating unit-load curve, while a TCN simultaneously extracted the temporal features of the zero AM load and daily average load. This divide-and-conquer mechanism generates multiple preliminary prediction results, which ultimately need to be synthesized into a final prediction using weighted averaging, voting mechanisms, or attention mechanisms. Hua et al.Â (Hua etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib87)) used an attention mechanism to dynamically connect the preliminary results, with a CNN capturing spatial features and a GRU capturing temporal features.

Unlike existing ELF methods that place separate convolutional layers on top of the entire RNN, Liu et al.Â (Liu etÂ al., [2022b](https://arxiv.org/html/2408.16202v1#bib.bib119)) embedded a 3-D convolutional filter within the LSTM unit, enabling the capture of translation-invariant local patterns both within and across spatial neighborhoods in the channels. This strategy of embedding one deep-learning model into another, through hierarchical model integration, enabled deep abstraction and effective utilization of data features. Eskandari et al.Â (Eskandari etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib45)) also used this strategy, combining a GRU and an LSTM to form a bidirectionally-propagating neural network. This used multidimensional features extracted by a 2-D CNN as input and provided these features to the bidirectional units for hourly ELF.

###  8.2. Probabilistic Forecasting Models

Probabilistic ELF predicts future demand using the uncertainty and randomness of the load. Unlike traditional deterministic methods, probabilistic models provide a quantified expression of predictive uncertainty, which has significant advantages for power grid planning and operationÂ (Yang etÂ al., [2018](https://arxiv.org/html/2408.16202v1#bib.bib187)). Probabilistic models express the uncertainty of prediction results by generating a distribution of outcomes, rather than a single valueÂ (Jalali etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib93)).

There are two main approaches for probabilistic ELF: parametric and non-parametric methods(Hou etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib82)). Parametric methods are based on assumptions about the distribution of the load data, typically assuming that the data follows a known probability distribution (such as the normal distribution or the Poisson distribution). Due to their use of fewer assumptions, non-parametric methods have much broader applicabilityÂ (VanÂ der Meer etÂ al., [2018](https://arxiv.org/html/2408.16202v1#bib.bib160)). Non-parametric methods do not require that the data follow a specific distribution form, starting instead directly from the actual data, uncovering the probabilistic distribution characteristics through the data itselfÂ (Huang etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib90)). To the best of our knowledge, the application of deep learning in probabilistic ELF extends from deterministic point predictions to probabilistic distribution forecastingÂ (Feng etÂ al., [2019](https://arxiv.org/html/2408.16202v1#bib.bib52); Cheng etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib28); Chen etÂ al., [2018](https://arxiv.org/html/2408.16202v1#bib.bib26)). This process requires the generation of point predictions, and then uses non-parametric techniques (such as quantile regression, bootstrapping, confidence interval estimation, gradient boosting, and kernel density estimationÂ (Wang etÂ al., [2019b](https://arxiv.org/html/2408.16202v1#bib.bib165))) to construct a probabilistic ELF model.

Lin et al.Â (Lin etÂ al., [2022a](https://arxiv.org/html/2408.16202v1#bib.bib114)) introduced an LSTM with a two-stage attention mechanism for probabilistic short-term regional load forecasting, enhancing uncertainty estimation and accuracy when trained with quantile loss. The combination of deep learning and non-parametric methods is an innovative solution to probabilistic ELF. This harnesses the deep-learning ability to capture data complexity and generate accurate point predictions, and enables forecasts with probability distributions through non-parametric techniques. Wang et al.Â (Wang etÂ al., [2019a](https://arxiv.org/html/2408.16202v1#bib.bib173)) extended the traditional LSTM-based point prediction to a quantile-based probabilistic forecast. Liu et al.Â (Liu etÂ al., [2022a](https://arxiv.org/html/2408.16202v1#bib.bib118)) integrated the GRU deep-feature-extraction capability with the CNNâ€™s efficient parallel processing, while also employing kernel density estimation to accurately fit the probability density. Zhang et al.Â (Zhang etÂ al., [2023e](https://arxiv.org/html/2408.16202v1#bib.bib197)) proposed the Quantile Regression Convolutional Bidirectional Long Short-Term Memory (QRCNNBiLSTM), which integrates quantile regression with feature extraction and bidirectional data processing. Through this integration, QRCNNBiLSTM can make precise joint predictions for the upper and lower bounds of the forecast interval.

The integration of deep learning and non-parametric methods in probabilistic ELF can more accurately capture the uncertainty in load variations, providing more comprehensive and reliable decision support for power systems.

##  9\. Answer to RQ6: Optimizing the Training Process 

This section provides the answers to RQ6, examining ways to optimize deep-learning training processesÂ (Zhang etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib201)).

Optimization mainly involves two aspects: network-structure optimization and error optimization. Optimization of the network structure relates to the architectural design of the model, and involves adjusting the number of layers, configuring the neurons, and modifying the connection methods. This all aims at constructing a robust model capable of capturing the complex features of the data. Error optimization involves thorough analysis and fine-tuning of the prediction errors, aiming to minimize the discrepancy between the modelâ€™s predictions and the actual observed values, thereby enhancing the accuracy and reliability.

###  9.1. Network-Structure Optimization

Network architecture design is a very important task, requiring careful selection of the number of neurons in each layer and the number of hidden layers in the network, etcÂ (Jiang and Zheng, [2022](https://arxiv.org/html/2408.16202v1#bib.bib95)). The selection is not completed in a single step, but rather needs to be determined based on the nature of the problem, the characteristics of the data, and the expected performance of the model. Selecting the optimal network structure and model parameters is a complex process, with a number of trial-and-error methods and heuristic optimization algorithms having been proposedÂ (Wang etÂ al., [2019b](https://arxiv.org/html/2408.16202v1#bib.bib165)).

As a fundamental problem-solving method, the trial-and-error approach involves gradually approaching a solution through continuous attempts and adjustments. This is not limited to only simple experiments and adjustments, but has been combined with heuristic optimization techniques to form efficient and systematic optimization strategies. The heuristic algorithms draw inspiration from optimization mechanisms found in nature and social phenomena, including Particle Swarm Optimization (PSO)Â (Hong and Chan, [2023](https://arxiv.org/html/2408.16202v1#bib.bib75)), Genetic Algorithms (GAs)Â (Dong etÂ al., [2021b](https://arxiv.org/html/2408.16202v1#bib.bib41)), the Whale Optimization Algorithm (WOA)Â (Haiyan etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib65)), Grey Wolf Optimizer (GWO)Â (Sekhar and Dahiya, [2023](https://arxiv.org/html/2408.16202v1#bib.bib145)), and the Grasshopper Optimization Algorithm (GOA)Â (Hu etÂ al., [2022b](https://arxiv.org/html/2408.16202v1#bib.bib83)).

Methods based on Bayesian optimizationÂ (Bayram etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib14); Xu etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib182)) predict parameter performance using probabilistic models, guiding the search process to more intelligently select the next set of candidate parameters. This is particularly well-suited for high-dimensional parameter spaces, significantly reducing the number of evaluations required. In summary, more accurate optimization algorithms can help to more effectively explore the potentially vast parameter space and identify parameter combinations that significantly enhance performance.

###  9.2. Error Optimization

Once the loss function is defined, the network weights need to be updated by calculating the loss-function gradient using optimization algorithms, such as gradient descent and its variants (Adam, RMSprop, etc.Â (Wu etÂ al., [2023a](https://arxiv.org/html/2408.16202v1#bib.bib175); Ganjouri etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib55)))Â (Hong and Fan, [2016](https://arxiv.org/html/2408.16202v1#bib.bib74)). These algorithms adjust the modelâ€™s parameters to gradually reduce the loss-function value, improving the modelâ€™s accuracy. The following is a list of some commonly-used optimizers:

  * â€¢

SGDÂ (Sakib etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib142)): This uses only one sample to compute the gradient and update the parameters in each iteration. This allows for quick updates, but can result in high variance, due to only using a single sample.

  * â€¢

RMSpropÂ (Langevin etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib105); Yazici etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib188); He, [2017](https://arxiv.org/html/2408.16202v1#bib.bib70)): This adjusts the learning rate by maintaining a decaying average of the squared gradients for each parameter, achieving adaptive learning rates. This is particularly effective for handling different learning rates for different parameters, accelerating convergence, and avoiding local minima or saddle points.

  * â€¢

AdaBeliefÂ (Yu etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib191)): This employs a concept of belief, which depends on the ratio of the squared gradient to its historical mean. When this ratio is greater than 1, AdaBelief is more inclined to trust the current gradient information; otherwise, it relies more on previous information.

  * â€¢

AdamÂ (Wang etÂ al., [2023a](https://arxiv.org/html/2408.16202v1#bib.bib166); Su etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib150); Sun etÂ al., [2020](https://arxiv.org/html/2408.16202v1#bib.bib152)): This is an adaptive gradient descent method that independently adjusts the learning rate for each parameter by combining the exponentially weighted averages of the first and second moments. This approach achieves a fast and robust optimization process.




##  10\. Answer to RQ7: Evaluation of Forecast Results 

This section provides the answers to RQ7, examining ways to evaluate the accuracy of the prediction results. This not only involves verifying the accuracy of the model outputs (to ensure that the predicted results are close to the actual load values), but also relates to the modelâ€™s reliability and effectiveness.

A comprehensive evaluation of the prediction model performance requires a set of evaluation metrics. These metrics should be able to quantify the size and distribution of prediction errors from different perspectives. They should enable a fair and objective comparison of the prediction performance of different models, under a unified standard. Metrics for the evaluation of deterministic load forecasting include: Mean Squared Error (MSE); Root Mean Squared Error (RMSE); Mean Absolute Error (MAE); Mean Absolute Percentage Error (MAPE); and R2superscriptğ�‘…2R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Similarly, metrics for the evaluation of probabilistic load forecasting include: Continuous Ranked Probability Score (CRPS); Prediction Interval Coverage Probability (PICP); and Pinball Loss (PL). Typically, multiple evaluation metrics are used together to enable a multi-faceted assessment. TableÂ [5](https://arxiv.org/html/2408.16202v1#S10.T5 "Table 5 â€£ 10. Answer to RQ7: Evaluation of Forecast Results â€£ Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey") lists the formulas and descriptions for some commonly-used evaluation metrics.

Table 5. Common Evaluation Metrics

Evaluation Metrics Formula Description Purpose MSE 1nâ�¢âˆ‘i=1n(yiâˆ’y^i)21ğ�‘›superscriptsubscriptğ�‘–1ğ�‘›superscriptsubscriptğ�‘¦ğ�‘–subscript^ğ�‘¦ğ�‘–2\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}divide start_ARG 1 end_ARG start_ARG italic_n end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Measures the difference between predicted values and actual values, focusing on large errors Deterministic Forecasting RMSE 1nâ�¢âˆ‘i=1n(yiâˆ’y^i)21ğ�‘›superscriptsubscriptğ�‘–1ğ�‘›superscriptsubscriptğ�‘¦ğ�‘–subscript^ğ�‘¦ğ�‘–2\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}}square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_n end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG The square root of the MSE, preserving its properties while also keeping consistent with the original data, making interpretation easier MAE 1nâ�¢âˆ‘i=1n|yiâˆ’y^i|1ğ�‘›superscriptsubscriptğ�‘–1ğ�‘›subscriptğ�‘¦ğ�‘–subscript^ğ�‘¦ğ�‘–\frac{1}{n}\sum_{i=1}^{n}|y_{i}-\hat{y}_{i}|divide start_ARG 1 end_ARG start_ARG italic_n end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT | italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | Measures the average absolute difference between predicted and actual values, focusing on small errors MAPE 1nâ�¢âˆ‘i=1n|yiâˆ’y^iyi|Ã—100%1ğ�‘›superscriptsubscriptğ�‘–1ğ�‘›subscriptğ�‘¦ğ�‘–subscript^ğ�‘¦ğ�‘–subscriptğ�‘¦ğ�‘–percent100\frac{1}{n}\sum_{i=1}^{n}\left|\frac{y_{i}-\hat{y}_{i}}{y_{i}}\right|\times 100\%divide start_ARG 1 end_ARG start_ARG italic_n end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT | divide start_ARG italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG | Ã— 100 % Measures the error as a percentage relative to the actual values, suitable for proportional data RÂ² 1âˆ’âˆ‘i=1n(yiâˆ’y^i)2âˆ‘i=1n(yiâˆ’yÂ¯)21superscriptsubscriptğ�‘–1ğ�‘›superscriptsubscriptğ�‘¦ğ�‘–subscript^ğ�‘¦ğ�‘–2superscriptsubscriptğ�‘–1ğ�‘›superscriptsubscriptğ�‘¦ğ�‘–Â¯ğ�‘¦21-\frac{\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}}{\sum_{i=1}^{n}(y_{i}-\overline{% y})^{2}}1 - divide start_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - overÂ¯ start_ARG italic_y end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG Assesses the modelâ€™s explanatory power, with values closer to 1 indicating a superior fit to the data CRPS âˆ«âˆ’âˆ�âˆ�(Fâ�¢(x)âˆ’ğ�Ÿ�{xâ‰¥y})2â�¢ğ�‘‘xsuperscriptsubscriptsuperscriptğ��¹ğ�‘¥subscript1ğ�‘¥ğ�‘¦2differential-dğ�‘¥\int_{-\infty}^{\infty}\left(F(x)-\mathbf{1}_{\\{x\geq y\\}}\right)^{2}dxâˆ« start_POSTSUBSCRIPT - âˆ� end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ� end_POSTSUPERSCRIPT ( italic_F ( italic_x ) - bold_1 start_POSTSUBSCRIPT { italic_x â‰¥ italic_y } end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d italic_x Measures the difference between the probabilistic forecast distribution and the actual observations Probabilistic Forecasting PICP 1Nâ�¢âˆ‘i=1Nğ�Ÿ�{aiâ‰¤yiâ‰¤bi}1ğ�‘�superscriptsubscriptğ�‘–1ğ�‘�subscript1subscriptğ�‘�ğ�‘–subscriptğ�‘¦ğ�‘–subscriptğ�‘�ğ�‘–\frac{1}{N}\sum_{i=1}^{N}\mathbf{1}_{\\{a_{i}\leq y_{i}\leq b_{i}\\}}divide start_ARG 1 end_ARG start_ARG italic_N end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT bold_1 start_POSTSUBSCRIPT { italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â‰¤ italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â‰¤ italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } end_POSTSUBSCRIPT Assesses how often the prediction intervals contain the actual observations PL 1nâ�¢âˆ‘i=1n(Ï„â�¢(yiâˆ’y^i)â�¢ğ�Ÿ�{yiâ‰¥y^i}+(1âˆ’Ï„)â�¢(y^iâˆ’yi)â�¢ğ�Ÿ�{yi<y^i})1ğ�‘›superscriptsubscriptğ�‘–1ğ�‘›ğ�œ�subscriptğ�‘¦ğ�‘–subscript^ğ�‘¦ğ�‘–subscript1subscriptğ�‘¦ğ�‘–subscript^ğ�‘¦ğ�‘–1ğ�œ�subscript^ğ�‘¦ğ�‘–subscriptğ�‘¦ğ�‘–subscript1subscriptğ�‘¦ğ�‘–subscript^ğ�‘¦ğ�‘–\frac{1}{n}\sum_{i=1}^{n}\left(\tau(y_{i}-\hat{y}_{i})\mathbf{1}_{\\{y_{i}\geq% \hat{y}_{i}\\}}+(1-\tau)(\hat{y}_{i}-y_{i})\mathbf{1}_{\\{y_{i}<\hat{y}_{i}\\}}\right)divide start_ARG 1 end_ARG start_ARG italic_n end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_Ï„ ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) bold_1 start_POSTSUBSCRIPT { italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â‰¥ over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } end_POSTSUBSCRIPT + ( 1 - italic_Ï„ ) ( over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) bold_1 start_POSTSUBSCRIPT { italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT < over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } end_POSTSUBSCRIPT ) Assesses the effectiveness of quantile predictions

Recently, some novel and improved evaluation metrics have also been introduced. Faustine et al.Â (Faustine and Pereira, [2022](https://arxiv.org/html/2408.16202v1#bib.bib50)) and Ganjouri et al.Â (Ganjouri etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib55)), for example, used a Normalized Root Mean Square Error (NRMSE) as evaluation metrics; Wang et al.Â (Wang etÂ al., [2023c](https://arxiv.org/html/2408.16202v1#bib.bib167)) used an Average Interval Score (AIS), Coverage Probability (CP), and Specificity Probability (SP) to evaluate the performance of interval probabilistic forecasting; and Zhang et al.Â (Zhang etÂ al., [2023g](https://arxiv.org/html/2408.16202v1#bib.bib204)) introduced the Coverage Rate (CR) to evaluate the predictive coverage of the model (which represents the proportion of times the confidence intervals generated by the model cover the true values out of the total number of instances) and the Interval Average Convergence (IAC) to assess the modelâ€™s convergence.

##  11\. Answer to RQ8: Challenges and Future Development Trends of STELF 

This section provides the answers to RQ8, examining the challenges and potential future opportunities for the application of deep learning in STELF.

###  11.1. Challenges

Although there has been significant effort dedicated to exploring the application of deep learning in STELF, especially in recent years, some challenges remain. Some of these challenges include: the need for standardized datasets; the generalizability of models; insufficient research in probabilistic load forecasting; the interpretability of deep learning results; and real-time prediction capabilities. Addressing and overcoming these challenges will be critical for the widespread application of deep learning in the field of STELF. In particular:

  * â€¢

STELF researchers use a variety of datasets, including both private and public data. This can present a challenge for verifying model performance. The lack of standardized benchmark datasets makes it complex and difficult to compare performances. Developing some widely recognized standardized datasets is thus of urgent importance for advancing STELF research.

  * â€¢

Although many studies have addressed the common goal of ELF, they often focus on different application scenarios and loads, such as building loadsÂ (Chiu etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib30)), household loadsÂ (Fekri etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib51)), and city-level loadsÂ (Yang etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib184)). The specificity of these scenarios may lead to models being overly optimized for a particular environment, raising challenges for the modelâ€™s generalizability. Therefore, It is necessary to study models with strong generalization ability for use in different scenarios.

  * â€¢

Although the academic community has extensively studied deterministic STELF, there remains a lack of research into probabilistic load forecasting. Compared with deterministic forecasting, there is significantly less focus on deep-learning-based probabilistic ELF models. Therefore, more research needs to be applied to probabilistic forecasting models, which will enhance their ability to cope with uncertainties.

  * â€¢

The interpretability of deep-learning-based forecasting models is an unresolved challenge. The complex mechanisms and decision pathways used by these models are often unknown, and lack intuitive transparency. This can impact the acceptance and effectiveness of the models in practical applications. Therefore, improving the interpretability of deep learning is crucial for its widespread adoption, long-term operation, and decision support in power systems.

  * â€¢

The demand for STELF is increasing, especially for real-time capabilities. However, most previous research is based on offline learning modes, which rely on large amounts of historical data for training, and may not easily incorporate the latest data for real-time learningÂ (Eren and KÃ¼Ã§Ã¼kdemiral, [2024](https://arxiv.org/html/2408.16202v1#bib.bib44)). To address this challenge, online learning mechanisms will be essential.




###  11.2. Research Trends

Recent STELF research trends include moving towards better integration, precision, and intelligence. Ongoing work to develop more sophisticated and enhanced forecasting models aims at improving prediction accuracy and reliability. This section examines the future STELF research directions of image-processing techniques, Large Language Models (LLMs), and optimization.

  * â€¢

As power networks evolve, dynamic graph models can adapt to changes in nodes and edges, maintaining the flexibility of the forecasting model. Graph techniques can integrate multiple data sources (such as geographical locations, historical loads, and weather conditions). Our review found that most STELF graph techniques use convolution operations to extract temporal or spatial feature relationships. However, Liu et al.Â (Liu etÂ al., [2022b](https://arxiv.org/html/2408.16202v1#bib.bib119)) converted the data into an image, and part of the future values to be predicted was transformed into blank patches. Thus, estimating future values became a similar problem to generating pixels for the missing regions of an image. Similar to image inpainting techniques, image generation and segmentation technologies have not yet been widely applied in STELF. Although these techniques may seem unrelated to STELF, their core ideas and methodologies can provide new perspectives and possibilities for STELF research. By applying these key image-processing concepts and algorithms to STELF, new research directions can be created, and modelsâ€™ ability to identify and predict complex load patterns can be enhanced.

  * â€¢

The recent rapid development of LLM technology has led to the exploration of its application to time-series forecasting tasks, opening up new areas in time-series predictionÂ (Yu etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib193); Chang etÂ al., [2023](https://arxiv.org/html/2408.16202v1#bib.bib22)). However, the use of LLMs for STELF has not yet been developed. Nevertheless, it is anticipated that application of LLMs to STELF shall become an important research direction. This may provide us with new ways to address the challenge of insufficient generalizability of forecasting models, while also opening up the possibility to achieve zero-shot learning in ELFÂ (Gruver etÂ al., [2024](https://arxiv.org/html/2408.16202v1#bib.bib60)). This type of model may not only demonstrate stronger adaptability and predictive power on diverse datasets, but also make reasonable predictions on unseen data.

  * â€¢

Optimization is a critical stage in the development of deep-learning models, and has been receiving attention in STELF Â (Zhang etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib201)). Future research will continue to explore more efficient optimization algorithms, such as improved SSAÂ (Neeraj etÂ al., [2021](https://arxiv.org/html/2408.16202v1#bib.bib130)), AdamÂ (Hong etÂ al., [2022](https://arxiv.org/html/2408.16202v1#bib.bib76)), and other optimization methods tailored to specific problems. Network architecture optimization is evolving towards more lightweight networks, with the relevant error-optimization process becoming more refined (including in-depth research and customization of loss functions). Model compression techniques and acceleration algorithms will continue to evolve, leading the optimization process to place greater emphasis on computational efficiency.




##  12\. Conclusion 

This review paper has examined advances in the application of deep learning in STELF over the past decade. Over this period, the application of deep learning to STELF has grown in popularity, and will continue to do so. We employed a comprehensive review-research methodology to identify the relevant literature, ensuring the breadth and depth of the research findings. We used specific keywords to search six major databases and conducted manual screening to ensure the completeness of the data. During the literature review process, we specifically extracted and recorded key information from each paper based on the designed eight RQs. We carefully organized the extracted information and created specific charts and tables to help readers understand.

In this paper, we have analyzed and summarized the search results, and provided a detailed analysis of publication trends. We organized the structure of the paper according to the practical workflow of STELF, (including the introduction of the dataset, data preprocessing methods, feature extraction methods, the introduction of deep learning models, optimization methods, and evaluation metrics) and conducted an in-depth analysis of each stage. The content of each prediction step is categorized according to a specific method to ensure it is presented in an organized manner. We have provided concise explanations of commonly-used techniques in conjunction with cited literature. This structured presentation makes the content clear and logical, while also helping researchers quickly understand the research trends and core issues in this domain. We have also summarized the challenges and future research trends in the STELF field. Overall, this review paper, with its comprehensive, systematic approach, and guidance, provides significant academic value and practical relevance.

## References

  * (1)
  * Abdel-Basset etÂ al. (2022) Mohamed Abdel-Basset, Hossam Hawash, Karam Sallam, SamehÂ S Askar, and Mohamed Abouhawwash. 2022.  STLF-Net: Two-stream deep network for short-term load forecasting in residential buildings.  _Journal of King Saud University-Computer and Information Sciences_ 34, 7 (2022), 4296â€“4311. 
  * Ahajjam etÂ al. (2022) MohamedÂ Aymane Ahajjam, DanielÂ Bonilla Licea, Mounir Ghogho, and Abdellatif Kobbane. 2022.  Experimental investigation of variational mode decomposition and deep learning for short-term multi-horizon residential electric load forecasting.  _Applied Energy_ 326 (2022), 119963. 
  * Akhtar etÂ al. (2023) Saima Akhtar, Sulman Shahzad, Asad Zaheer, HafizÂ Sami Ullah, Heybet Kilic, Radomir Gono, MichaÅ‚ JasiÅ„ski, and Zbigniew Leonowicz. 2023.  Short-term load forecasting models: A review of challenges, progress, and the road ahead.  _Energies_ 16, 10 (2023), 4060. 
  * AlÂ Mamun etÂ al. (2020) Abdullah AlÂ Mamun, Md Sohel, Naeem Mohammad, MdÂ SamiulÂ Haque Sunny, DebopriyaÂ Roy Dipta, and Eklas Hossain. 2020.  A comprehensive review of the load forecasting techniques using single and hybrid predictive models.  _IEEE Access_ 8 (2020), 134911â€“134939. 
  * Alipour etÂ al. (2020) Mohammadali Alipour, Jamshid Aghaei, Mohammadali Norouzi, Taher Niknam, Sattar Hashemi, and Matti Lehtonen. 2020.  A novel electrical net-load forecasting model based on deep neural networks and wavelet transform integration.  _Energy_ 205 (2020), 118106. 
  * Almalaq and Edwards (2017) Abdulaziz Almalaq and George Edwards. 2017.  A review of deep learning methods applied on load forecasting. In _Proceedings of the 16th IEEE International Conference on Machine Learning and Applications (ICMLAâ€™17)_. 511â€“516. 
  * Aouad etÂ al. (2022) Mosbah Aouad, Hazem Hajj, Khaled Shaban, RabihÂ A Jabr, and Wassim El-Hajj. 2022.  A CNN-Sequence-to-Sequence network with attention for residential short-term load forecasting.  _Electric Power Systems Research_ 211 (2022), 108152. 
  * Arastehfar etÂ al. (2022) Sana Arastehfar, Mohammadjavad Matinkia, and MohammadÂ Reza Jabbarpour. 2022.  Short-term residential load forecasting using graph convolutional recurrent neural networks.  _Engineering Applications of Artificial Intelligence_ 116 (2022), 105358. 
  * Aseeri (2023) AhmadÂ O Aseeri. 2023.  Effective RNN-based forecasting methodology design for improving short-term power load forecasts: Application to large-scale power-grid time series.  _Journal of Computational Science_ 68 (2023), 101984. 
  * Atef and Eltawil (2020) Sara Atef and AmrÂ B Eltawil. 2020.  Assessment of stacked unidirectional and bidirectional long short-term memory networks for electricity load forecasting.  _Electric Power Systems Research_ 187 (2020), 106489. 
  * Bai etÂ al. (2018) Shaojie Bai, JÂ Zico Kolter, and Vladlen Koltun. 2018.  An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.  _arXiv preprint arXiv:1803.01271_ (2018). 
  * Bashir etÂ al. (2022) Tasarruf Bashir, Chen Haoyong, MuhammadÂ Faizan Tahir, and Zhu Liqiang. 2022.  Short term electricity load forecasting using hybrid prophet-LSTM model optimized by BPNN.  _Energy reports_ 8 (2022), 1678â€“1686. 
  * Bayram etÂ al. (2023) Firas Bayram, Phil Aupke, BestounÂ S Ahmed, Andreas Kassler, Andreas Theocharis, and Jonas Forsman. 2023.  DA-LSTM: A dynamic drift-adaptive learning framework for interval load forecasting with LSTM networks.  _Engineering Applications of Artificial Intelligence_ 123 (2023), 106480. 
  * Benidis etÂ al. (2022) Konstantinos Benidis, SyamaÂ Sundar Rangapuram, Valentin Flunkert, Yuyang Wang, Danielle Maddix, Caner Turkmen, Jan Gasthaus, Michael Bohlke-Schneider, David Salinas, Lorenzo Stella, etÂ al. 2022\.  Deep learning for time series forecasting: Tutorial and literature survey.  _Comput. Surveys_ 55, 6 (2022), 1â€“36. 
  * Bian etÂ al. (2022a) Haihong Bian, Qian Wang, Guozheng Xu, and Xiu Zhao. 2022a.  Load forecasting of hybrid deep learning model considering accumulated temperature effect.  _Energy Reports_ 8 (2022), 205â€“215. 
  * Bian etÂ al. (2022b) Haihong Bian, Qian Wang, Guozheng Xu, and Xiu Zhao. 2022b.  Research on short-term load forecasting based on accumulated temperature effect and improved temporal convolutional network.  _Energy Reports_ 8 (2022), 1482â€“1491. 
  * Bu etÂ al. (2023) Xiangya Bu, Qiuwei Wu, Bin Zhou, and Canbing Li. 2023.  Hybrid short-term load forecasting using CGAN with CNN and semi-supervised regression.  _Applied Energy_ 338 (2023), 120920. 
  * Bunn and Farmer (1985) D Bunn and EÂ Dillon Farmer. 1985.  Comparative models for electrical load forecasting.  (1985). 
  * Cai etÂ al. (2020) Qiuna Cai, Binjie Yan, Binghong Su, Sijie Liu, Mingxu Xiang, Yakun Wen, Yanyu Cheng, and Nan Feng. 2020.  Short-term load forecasting method based on deep neural network with sample weights.  _International Transactions on Electrical Energy Systems_ 30, 5 (2020), e12340. 
  * Chandola etÂ al. (2009) Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009.  Anomaly detection: A survey.  _ACM computing surveys (CSUR)_ 41, 3 (2009), 1â€“58. 
  * Chang etÂ al. (2023) Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. 2023.  Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained LLMs.  _arXiv preprint arXiv:2308.08469_ (2023). 
  * Chaturvedi etÂ al. (2015) DK Chaturvedi, AP Sinha, and OP Malik. 2015.  Short term load forecast using fuzzy logic and wavelet transform integrated generalized neural network.  _International Journal of Electrical Power & Energy Systems_ 67 (2015), 230â€“237. 
  * Chen etÂ al. (2019) Haiwen Chen, Shouxiang Wang, Shaomin Wang, and Ye Li. 2019.  Day-ahead aggregated load forecasting based on two-terminal sparse coding and deep neural network fusion.  _Electric Power Systems Research_ 177 (2019), 105987. 
  * Chen etÂ al. (2023) Houhe Chen, Mingyang Zhu, Xiao Hu, Jiarui Wang, Yong Sun, and Jinduo Yang. 2023.  Research on short-term load forecasting of new-type power system based on GCN-LSTM considering multiple influencing factors.  _Energy Reports_ 9 (2023), 1022â€“1031. 
  * Chen etÂ al. (2018) Kunjin Chen, Kunlong Chen, Qin Wang, Ziyu He, Jun Hu, and Jinliang He. 2018.  Short-term load forecasting with deep residual networks.  _IEEE Transactions on Smart Grid_ 10, 4 (2018), 3943â€“3952. 
  * Chen etÂ al. (2021) Zexi Chen, Delong Zhang, Haoran Jiang, Longze Wang, Yongcong Chen, Yang Xiao, Jinxin Liu, Yan Zhang, and Meicheng Li. 2021.  Load forecasting based on LSTM neural network and applicable to loads of â€œreplacement of coal with electricityâ€�.  _Journal of Electrical Engineering & Technology_ 16, 5 (2021), 2333â€“2342. 
  * Cheng etÂ al. (2021) Lilin Cheng, Haixiang Zang, Yan Xu, Zhinong Wei, and Guoqiang Sun. 2021.  Probabilistic residential load forecasting based on micrometeorological data and customer consumption pattern.  _IEEE Transactions on Power systems_ 36, 4 (2021), 3762â€“3775. 
  * Cheung etÂ al. (2021) ChungÂ Ming Cheung, Sanmukh Kuppannagari, Rajgopal Kannan, and ViktorÂ K Prasanna. 2021.  Leveraging spatial information in smart grids using STGCN for short-term load forecasting. In _Proceedings of the 13th International Conference on Contemporary Computing (IC3â€™21)_. 159â€“167. 
  * Chiu etÂ al. (2023) Ming-Chuan Chiu, Hsin-Wei Hsu, Ke-Sin Chen, and Chih-Yuan Wen. 2023.  A hybrid CNN-GRU based probabilistic model for load forecasting from individual household to commercial building.  _Energy Reports_ 9 (2023), 94â€“105. 
  * Choi etÂ al. (2018) Hyungeun Choi, Seunghyoung Ryu, and Hongseok Kim. 2018.  Short-term load forecasting based on ResNet and LSTM. In _Proceedings of the 2018 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridCommâ€™18)_. 1â€“6. 
  * Dai etÂ al. (2023) Yeming Dai, Xinyu Yang, and Mingming Leng. 2023.  Optimized Seq2Seq model based on multiple methods for short-term power load forecasting.  _Applied Soft Computing_ 142 (2023), 110335. 
  * Das etÂ al. (2020) Anooshmita Das, MasabÂ Khalid Annaqeeb, Elie Azar, Vojislav Novakovic, and MikkelÂ Baun KjÃ¦rgaard. 2020.  Occupant-centric miscellaneous electric loads prediction in buildings using state-of-the-art deep learning methods.  _Applied Energy_ 269 (2020), 115135. 
  * Dedinec etÂ al. (2016) Aleksandra Dedinec, Sonja Filiposka, Aleksandar Dedinec, and Ljupco Kocarev. 2016.  Deep belief network based electricity load forecasting: An analysis of Macedonian case.  _Energy_ 115 (2016), 1688â€“1700. 
  * Deepanraj etÂ al. (2022) B Deepanraj, N Senthilkumar, T Jarin, AliÂ Etem Gurel, LÂ Syam Sundar, and AÂ Vivek Anand. 2022.  Intelligent wild geese algorithm with deep learning driven short term load forecasting for sustainable energy management in microgrids.  _Sustainable Computing: Informatics and Systems_ 36 (2022), 100813. 
  * Din and Marnerides (2017) Ghulam MohiÂ Ud Din and AngelosÂ K Marnerides. 2017.  Short term power load forecasting using deep neural networks. In _Proceedings of the 2017 International Conference on Computing, Networking and Communications (ICNCâ€™17)_. 594â€“598. 
  * Dogra etÂ al. (2023) Atharvan Dogra, Ashima Anand, and Jatin Bedi. 2023.  Consumers profiling based federated learning approach for energy load forecasting.  _Sustainable Cities and Society_ 98 (2023), 104815. 
  * Dong etÂ al. (2023) Jizhe Dong, Long Luo, Yu Lu, and Qi Zhang. 2023.  A parallel short-term power load forecasting method considering high-level elastic loads.  _IEEE Transactions on Instrumentation and Measurement_ 72 (2023), 1â€“10. 
  * Dong etÂ al. (2017) Xishuang Dong, Lijun Qian, and Lei Huang. 2017.  Short-term load forecasting in smart grid: A combined CNN and K-means clustering approach. In _Proceedings of the 2017 IEEE International Conference on Big Data and Smart Computing (BigCompâ€™17)_. 119â€“125. 
  * Dong etÂ al. (2021a) Yi Dong, Zhen Dong, Tianqiao Zhao, Zhongguo Li, and Zhengtao Ding. 2021a.  Short term load forecasting with markovian switching distributed deep belief networks.  _International Journal of Electrical Power & Energy Systems_ 130 (2021), 106942. 
  * Dong etÂ al. (2021b) Yunxuan Dong, Xuejiao Ma, and Tonglin Fu. 2021b.  Electrical load forecasting: A deep learning approach based on K-nearest neighbors.  _Applied Soft Computing_ 99 (2021), 106900. 
  * Dou etÂ al. (2018) Yuchen Dou, Xinman Zhang, Zhihui Wu, and Hang Zhang. 2018.  Application of deep learning method in short-term load forecasting of characteristic enterprises. In _Proceedings of the 2018 Artificial Intelligence and Cloud Computing Conference (AICCCâ€™18)_. 35â€“40. 
  * Dudek (2016) Grzegorz Dudek. 2016.  Neural networks for pattern-based short-term load forecasting: A comparative study.  _Neurocomputing_ 205 (2016), 64â€“74. 
  * Eren and KÃ¼Ã§Ã¼kdemiral (2024) Yavuz Eren and Ä°brahim KÃ¼Ã§Ã¼kdemiral. 2024.  A comprehensive review on deep learning approaches for short-term load forecasting.  _Renewable and Sustainable Energy Reviews_ 189 (2024), 114031. 
  * Eskandari etÂ al. (2021) Hosein Eskandari, Maryam Imani, and MohsenÂ Parsa Moghaddam. 2021.  Convolutional and recurrent neural network based model for short-term load forecasting.  _Electric Power Systems Research_ 195 (2021), 107173. 
  * Fahiman etÂ al. (2019) Fateme Fahiman, SarahÂ M Erfani, and Christopher Leckie. 2019.  Robust and accurate short-term load forecasting: A cluster oriented ensemble learning approach. In _Proceedings of the 2019 International Joint Conference on Neural Networks (IJCNNâ€™19)_. 1â€“8. 
  * Fahiman etÂ al. (2017) Fateme Fahiman, SarahÂ M Erfani, Sutharshan Rajasegarar, Marimuthu Palaniswami, and Christopher Leckie. 2017.  Improving load forecasting based on deep learning and K-shape clustering. In _Proceedings of the 2017 International Joint Conference on Neural Networks (IJCNNâ€™17)_. 4134â€“4141. 
  * Fan etÂ al. (2020) Chaodong Fan, Changkun Ding, Jinhua Zheng, Leyi Xiao, and Zhaoyang Ai. 2020.  Empirical mode decomposition based multi-objective deep belief network for short-term power load forecasting.  _Neurocomputing_ 388 (2020), 110â€“123. 
  * Farid etÂ al. (2023) KarimÂ S Farid, AA Ali, SamehÂ A Salem, and AmrÂ E Mohamed. 2023.  CONV1D-GRU: A hybrid model for short-term electrical load forecasting. In _Proceedings of the 2023 International Telecommunications Conference (ITC-Egyptâ€™23)_. 281â€“286. 
  * Faustine and Pereira (2022) Anthony Faustine and Lucas Pereira. 2022.  FPSeq2Q: Fully parameterized sequence to quantile regression for net-load forecasting with uncertainty estimates.  _IEEE Transactions on Smart Grid_ 13, 3 (2022), 2440â€“2451. 
  * Fekri etÂ al. (2021) MohammadÂ Navid Fekri, Harsh Patel, Katarina Grolinger, and Vinay Sharma. 2021.  Deep learning for load forecasting with smart meter data: Online adaptive recurrent neural network.  _Applied Energy_ 282 (2021), 116177. 
  * Feng etÂ al. (2019) Cong Feng, Mucun Sun, and Jie Zhang. 2019.  Reinforced deterministic and probabilistic load forecasting via Q-learning dynamic model selection.  _IEEE Transactions on Smart Grid_ 11, 2 (2019), 1377â€“1386. 
  * Feng etÂ al. (2024) Ding Feng, Dengao Li, Yu Zhou, Jumin Zhao, and Kenan Zhang. 2024.  STGNet: Short-term residential load forecasting with spatialâ€“temporal gated fusion network.  _Energy Science & Engineering_ 12, 3 (2024), 541â€“560. 
  * Gan etÂ al. (2017) Dahua Gan, Yi Wang, Ning Zhang, and Wenjun Zhu. 2017.  Enhancing short-term probabilistic residential load forecasting with quantile longâ€“short-term memory.  _The Journal of Engineering_ 2017, 14 (2017), 2622â€“2627. 
  * Ganjouri etÂ al. (2023) Mahtab Ganjouri, Mazda Moattari, Ahmad Forouzantabar, and Mohammad Azadi. 2023.  Spatial-temporal learning structure for short-term load forecasting.  _IET Generation, Transmission & Distribution_ 17, 2 (2023), 427â€“437. 
  * Gao etÂ al. (2023a) Jiaxin Gao, Yuntian Chen, Wenbo Hu, and Dongxiao Zhang. 2023a.  An adaptive deep-learning load forecasting framework by integrating transformer and domain knowledge.  _Advances in Applied Energy_ 10 (2023), 100142. 
  * Gao etÂ al. (2023b) Qiang Gao, Kaiyi Liu, Kaibin Wu, Menghan You, and Hang Liu. 2023b.  Short-term load forecasting for typical buildings based on VMD-Informer-DMD model. In _Proceedings of the IEEE 2nd Industrial Electronics Society Annual On-Line Conference (ONCONâ€™23)_. 1â€“6. 
  * Ghofrani etÂ al. (2015) Mahmoud Ghofrani, M Ghayekhloo, A Arabali, and A Ghayekhloo. 2015.  A hybrid short-term load forecasting with a new input selection framework.  _Energy_ 81 (2015), 777â€“786. 
  * Giacomazzi etÂ al. (2023) Elena Giacomazzi, Felix Haag, and Konstantin Hopf. 2023.  Short-term electricity load forecasting using the temporal fusion transformer: Effect of grid hierarchies and data sources. In _Proceedings of the 14th ACM International Conference on Future Energy Systems (e-Energyâ€™23)_. 353â€“360. 
  * Gruver etÂ al. (2024) Nate Gruver, Marc Finzi, Shikai Qiu, and AndrewÂ G Wilson. 2024.  Large language models are zero-shot time series forecasters.  _Advances in Neural Information Processing Systems_ 36 (2024). 
  * Guo etÂ al. (2021) Xifeng Guo, Ye Gao, Yupeng Li, Di Zheng, and Dan Shan. 2021.  Short-term household load forecasting based on long-and short-term time-series network.  _Energy Reports_ 7 (2021), 58â€“64. 
  * Guo etÂ al. (2020) Xifeng Guo, Qiannan Zhao, Di Zheng, Yi Ning, and Ye Gao. 2020.  A short-term load forecasting model of multi-scale CNN-LSTM hybrid neural network considering the real-time electricity price.  _Energy Reports_ 6 (2020), 1046â€“1053. 
  * GÃ¼rses-Tran etÂ al. (2022) Gonca GÃ¼rses-Tran, TobiasÂ Alexander KÃ¶rner, and Antonello Monti. 2022.  Introducing explainability in sequence-to-sequence learning for short-term load forecasting.  _Electric Power Systems Research_ 212 (2022), 108366. 
  * Hafeez etÂ al. (2020) Ghulam Hafeez, KhurramÂ Saleem Alimgeer, and Imran Khan. 2020.  Electric load forecasting based on deep learning and optimized by heuristic algorithm in smart grid.  _Applied Energy_ 269 (2020), 114915. 
  * Haiyan etÂ al. (2020) Wang Haiyan, Lv Xinhang, and Luo Xiaonan. 2020.  Short-term load forecasting of power grid based on improved WOA optimized LSTM. In _Proceedings of the 5th International Conference on Power and Renewable Energy (ICPREâ€™20)_. 54â€“60. 
  * Han etÂ al. (2023) Shuwei Han, Huitong Ru, Guangling Wang, Xuezhi Fu, Guoxu Zhou, and Chengqiao Yang. 2023.  Research on power load forecasting of PCA-CNN-LSTM based on sliding window. In _Proceedings of the 3rd International Conference on New Energy and Power Engineering (ICNEPEâ€™23)_. 466â€“471. 
  * Haque and Rahman (2022) Ashraful Haque and Saifur Rahman. 2022.  Short-term electrical load forecasting through heuristic configuration of regularized deep neural network.  _Applied Soft Computing_ 122 (2022), 108877. 
  * Hayati and Shirvany (2007) Mohsen Hayati and Yazdan Shirvany. 2007.  Artificial neural network approach for short term load forecasting for Illam region.  _World Academy of Science, Engineering and Technology_ 28 (2007), 280â€“284. 
  * He etÂ al. (2021) Shengtao He, Canbing Li, Xubin Liu, Xinyu Chen, Mohammad Shahidehpour, Tao Chen, Bin Zhou, and Qiuwei Wu. 2021.  A per-unit curve rotated decoupling method for CNN-TCN based day-ahead load forecasting.  _IET Generation, Transmission & Distribution_ 15, 19 (2021), 2773â€“2786. 
  * He (2017) Wan He. 2017.  Load forecasting via deep neural networks.  _Procedia Computer Science_ 122 (2017), 308â€“314. 
  * He etÂ al. (2017) Yusen He, Jiahao Deng, and Huajin Li. 2017.  Short-term power load forecasting with deep belief network and copula models. In _Proceedings of the 9th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSCâ€™17)_. 191â€“194. 
  * Hinton etÂ al. (2006) GeoffreyÂ E Hinton, Simon Osindero, and Yee-Whye Teh. 2006.  A fast learning algorithm for deep belief nets.  _Neural computation_ 18, 7 (2006), 1527â€“1554. 
  * Hochreiter and Schmidhuber (1997) Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997.  Long short-term memory.  _Neural computation_ 9, 8 (1997), 1735â€“1780. 
  * Hong and Fan (2016) Tao Hong and Shu Fan. 2016.  Probabilistic electric load forecasting: A tutorial review.  _International Journal of Forecasting_ 32, 3 (2016), 914â€“938. 
  * Hong and Chan (2023) Ying-Yi Hong and Yu-Hsuan Chan. 2023.  Short-term electric load forecasting using particle swarm optimization-based convolutional neural network.  _Engineering Applications of Artificial Intelligence_ 126 (2023), 106773. 
  * Hong etÂ al. (2022) Ying-Yi Hong, Yu-Hsuan Chan, Yung-Han Cheng, Yih-Der Lee, Jheng-Lun Jiang, and Shen-Szu Wang. 2022.  Week-ahead daily peak load forecasting using genetic algorithm-based hybrid convolutional neural network.  _IET Generation, Transmission & Distribution_ 16, 12 (2022), 2416â€“2424. 
  * Hoori etÂ al. (2019) AmmarÂ O Hoori, Ahmad AlÂ Kazzaz, Rameez Khimani, Yuichi Motai, and AlexÂ J Aved. 2019.  Electric load forecasting model using a multicolumn deep neural networks.  _IEEE Transactions on Industrial Electronics_ 67, 8 (2019), 6473â€“6482. 
  * Hosein and Hosein (2017) Stefan Hosein and Patrick Hosein. 2017.  Load forecasting using deep neural networks. In _Proceedings of the 2017 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGTâ€™17)_. 1â€“5. 
  * Hossain and Mahmood (2020) MohammadÂ Safayet Hossain and Hisham Mahmood. 2020.  Short-term load forecasting using an LSTM neural network. In _Proceedings of the 2020 IEEE Power and Energy Conference at Illinois (PECIâ€™20)_. 1â€“6. 
  * Hossen etÂ al. (2018) Tareq Hossen, ArunÂ Sukumaran Nair, RadhakrishnanÂ Angamuthu Chinnathambi, and Prakash Ranganathan. 2018.  Residential load forecasting using deep neural networks (DNN). In _Proceedings of the 2018 North American Power Symposium (NAPSâ€™18)_. 1â€“5. 
  * Hossen etÂ al. (2017) Tareq Hossen, SibyÂ Jose Plathottam, RadhaÂ Krishnan Angamuthu, Prakash Ranganathan, and Hossein Salehfar. 2017.  Short-term load forecasting using deep neural networks (DNN). In _Proceedings of the 2017 North American Power Symposium (NAPSâ€™17)_. 1â€“6. 
  * Hou etÂ al. (2022) Hui Hou, Chao Liu, Qing Wang, Xixiu Wu, Jinrui Tang, Ying Shi, and Changjun Xie. 2022.  Review of load forecasting based on artificial intelligence methodologies, models, and challenges.  _Electric Power Systems Research_ 210 (2022), 108067. 
  * Hu etÂ al. (2022b) Haowen Hu, Xin Xia, Yuanlin Luo, Chu Zhang, MuhammadÂ Shahzad Nazir, and Tian Peng. 2022b.  Development and application of an evolutionary deep learning framework of LSTM based on improved grasshopper optimization algorithm for short-term load forecasting.  _Journal of Building Engineering_ 57 (2022), 104975. 
  * Hu etÂ al. (2017) Rui Hu, Shiping Wen, Zhigang Zeng, and Tingwen Huang. 2017.  A short-term power load forecasting model based on the generalized regression neural network with decreasing step fruit fly optimization algorithm.  _Neurocomputing_ 221 (2017), 24â€“31. 
  * Hu etÂ al. (2022c) Weimin Hu, Chao Yan, Liping Fan, Jie Yu, Mei Yu, Sheng Hua, and Chonghao Yue. 2022c.  Short-term power load forecasting based on VMD-SSA-LSTM. In _Proceedings of the 2022 International Conference on High Performance Big Data and Intelligent Systems (HDISâ€™22)_. 287â€“293. 
  * Hu etÂ al. (2022a) Xin Hu, Keyi Li, Jingfu Li, Taotao Zhong, Weinong Wu, Xia Zhang, and Wenjiang Feng. 2022a.  Load forecasting model consisting of data mining based orthogonal greedy algorithm and long short-term memory network.  _Energy Reports_ 8 (2022), 235â€“242. 
  * Hua etÂ al. (2023) Heng Hua, Mingping Liu, Yuqin Li, Suhui Deng, and Qingnian Wang. 2023.  An ensemble framework for short-term load forecasting based on parallel CNN and GRU with improved ResNet.  _Electric Power Systems Research_ 216 (2023), 109057. 
  * Huang etÂ al. (2021) Jiehui Huang, Zhiwang Zhou, Chunquan Li, Zhiyuan Liao, and PeterÂ X Liu. 2021.  A decomposition-based multi-time dimension long short-term memory model for short-term electric load forecasting.  _IET Generation, Transmission & Distribution_ 15, 24 (2021), 3459â€“3473. 
  * Huang etÂ al. (2023) Nantian Huang, Shengyuan Wang, Rijun Wang, Guowei Cai, Yang Liu, and Qianbin Dai. 2023.  Gated spatial-temporal graph neural network based short-term load forecasting for wide-area multiple buses.  _International Journal of Electrical Power & Energy Systems_ 145 (2023), 108651. 
  * Huang etÂ al. (2020) Qian Huang, Jinghua Li, and Mengshu Zhu. 2020.  An improved convolutional neural network with load range discretization for probabilistic load forecasting.  _Energy_ 203 (2020), 117902. 
  * Huang etÂ al. (21) Rubing Huang, Weifeng Sun, Yinyin Xu, Haibo Chen, Dave Towey, and Xin Xia. 21.  A survey on adaptive random testing.  _IEEE Transactions on Software Engineering_ 47, 10 (21), 2052â€“2083. 
  * Jalali etÂ al. (2021) Seyed MohammadÂ Jafar Jalali, Sajad Ahmadian, Abbas Khosravi, Miadreza Shafie-khah, Saeid Nahavandi, and JoÃ£oÂ PS CatalÃ£o. 2021.  A novel evolutionary-based deep convolutional neural network model for intelligent load forecasting.  _IEEE Transactions on Industrial Informatics_ 17, 12 (2021), 8243â€“8253. 
  * Jalali etÂ al. (2022) Seyed MohammadÂ Jafar Jalali, Parul Arora, BK Panigrahi, Abbas Khosravi, Saeid Nahavandi, GerardoÂ J OsÃ³rio, and JoÃ£oÂ PS CatalÃ£o. 2022.  An advanced deep neuroevolution model for probabilistic load forecasting.  _Electric Power Systems Research_ 211 (2022), 108351. 
  * Javed etÂ al. (2022) Umar Javed, Khalid Ijaz, Muhammad Jawad, Ikramullah Khosa, EjazÂ Ahmad Ansari, KhurramÂ Shabih Zaidi, MuhammadÂ Nadeem Rafiq, and Noman Shabbir. 2022.  A novel short receptive field based dilated causal convolutional network integrated with Bidirectional LSTM for short-term load forecasting.  _Expert Systems with Applications_ 205 (2022), 117689. 
  * Jiang and Zheng (2022) He Jiang and Weihua Zheng. 2022.  Deep learning with regularized robust long-and short-term memory network for probabilistic short-term load forecasting.  _Journal of Forecasting_ 41, 6 (2022), 1201â€“1216. 
  * Jiao etÂ al. (2021) Runhai Jiao, Shuangkun Wang, Tianle Zhang, Hui Lu, Hui He, and BrijÂ B Gupta. 2021.  Adaptive feature selection and construction for day-ahead load forecasting use deep learning method.  _IEEE Transactions on Network and Service Management_ 18, 4 (2021), 4019â€“4029. 
  * Jin etÂ al. (2022) Yuwei Jin, MosesÂ Amoasi Acquah, Mingyu Seo, and Sekyung Han. 2022.  Short-term electric load prediction using transfer learning with interval estimate adjustment.  _Energy and Buildings_ 258 (2022), 111846. 
  * Khan etÂ al. (2022) ZulfiqarÂ Ahmad Khan, Amin Ullah, IjazÂ Ul Haq, Mohamed Hamdy, GerardoÂ Maria Mauro, Khan Muhammad, Mohammad Hijji, and SungÂ Wook Baik. 2022.  Efficient short-term electricity load forecasting for effective energy management.  _Sustainable Energy Technologies and Assessments_ 53 (2022), 102337. 
  * Kim etÂ al. (2019) Junhong Kim, Jihoon Moon, Eenjun Hwang, and Pilsung Kang. 2019.  Recurrent inception convolution neural network for multi short-term load forecasting.  _Energy and buildings_ 194 (2019), 328â€“341. 
  * Kim etÂ al. (2022) Nakyoung Kim, Hyunseo Park, Joohyung Lee, and JunÂ Kyun Choi. 2022.  Short-term electrical load forecasting with multidimensional feature extraction.  _IEEE Transactions on Smart Grid_ 13, 4 (2022), 2999â€“3013. 
  * Kong etÂ al. (2017) Weicong Kong, ZhaoÂ Yang Dong, Youwei Jia, DavidÂ J Hill, Yan Xu, and Yuan Zhang. 2017.  Short-term residential load forecasting based on LSTM recurrent neural network.  _IEEE Transactions on Smart Grid_ 10, 1 (2017), 841â€“851. 
  * Kong etÂ al. (2019) Xiangyu Kong, Chuang Li, Feng Zheng, and Chengshan Wang. 2019.  Improved deep belief network for short-term load forecasting considering demand-side management.  _IEEE Transactions on Power Systems_ 35, 2 (2019), 1531â€“1538. 
  * Kouhi etÂ al. (2014) Sajjad Kouhi, Farshid Keynia, and SajadÂ Najafi Ravadanegh. 2014.  A new short-term load forecast method based on neuro-evolutionary algorithm and chaotic feature selection.  _International Journal of Electrical Power & Energy Systems_ 62 (2014), 862â€“867. 
  * Lai etÂ al. (2020) ChunÂ Sing Lai, Zhenyao Mo, Ting Wang, Haoliang Yuan, WingÂ WY Ng, and LoiÂ Lei Lai. 2020.  Load forecasting based on deep neural network and historical data augmentation.  _IET Generation, Transmission & Distribution_ 14, 24 (2020), 5927â€“5934. 
  * Langevin etÂ al. (2023) Antoine Langevin, Mohamed Cheriet, and Ghyslain Gagnon. 2023.  Efficient deep generative model for short-term household load forecasting using non-intrusive load monitoring.  _Sustainable Energy, Grids and Networks_ 34 (2023), 101006. 
  * LeCun etÂ al. (2015) Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015.  Deep learning.  _Nature_ 521, 7553 (2015), 436â€“444. 
  * Li etÂ al. (2023) Bin Li, Yulu Mo, Feng Gao, and Xiaoqing Bai. 2023.  Short-term probabilistic load forecasting method based on uncertainty estimation and deep learning model considering meteorological factors.  _Electric Power Systems Research_ 225 (2023), 109804. 
  * Li etÂ al. (2019) Chen Li, Zhenyu Chen, Jinbo Liu, Dapeng Li, Xingyu Gao, Fangchun Di, Lixin Li, and Xiaohui Ji. 2019.  Power load forecasting based on the combined model of LSTM and XGBoost. In _Proceedings of the 2019 International Conference on Pattern Recognition and Artificial Intelligence (PRAIâ€™19)_. 46â€“51. 
  * Li etÂ al. (2022a) Dan Li, Guangfan Sun, Shuwei Miao, Yingzhong Gu, Yuanhang Zhang, and Shuai He. 2022a.  A short-term electric load forecast method based on improved sequence-to-sequence GRU with adaptive temporal dependence.  _International Journal of Electrical Power & Energy Systems_ 137 (2022), 107627. 
  * Li etÂ al. (2021) Lechen Li, ChristophÂ J Meinrenken, Vijay Modi, and PatriciaÂ J Culligan. 2021.  Short-term apartment-level load forecasting using a modified neural network with selected auto-regressive features.  _Applied Energy_ 287 (2021), 116509. 
  * Li etÂ al. (2020b) Ning Li, Lu Wang, Xinquan Li, and Qing Zhu. 2020b.  An effective deep learning neural network model for short-term load forecasting.  _Concurrency and Computation: Practice and Experience_ 32, 7 (2020), e5595. 
  * Li etÂ al. (2022b) Xiaole Li, Yiqin Wang, Guibo Ma, Xin Chen, Qianxiang Shen, and Bo Yang. 2022b.  Electric load forecasting based on Long-Short-Term-Memory network via simplex optimizer during COVID-19.  _Energy Reports_ 8 (2022), 1â€“12. 
  * Li etÂ al. (2020a) Zhuoling Li, Yuanzheng Li, Yun Liu, Ping Wang, Renzhi Lu, and HoayÂ Beng Gooi. 2020a.  Deep learning based densely connected network for load forecasting.  _IEEE Transactions on Power Systems_ 36, 4 (2020), 2829â€“2840. 
  * Lin etÂ al. (2022a) Jun Lin, Jin Ma, Jianguo Zhu, and Yu Cui. 2022a.  Short-term load forecasting based on LSTM networks considering attention mechanism.  _International Journal of Electrical Power & Energy Systems_ 137 (2022), 107818. 
  * Lin etÂ al. (2021) Weixuan Lin, Di Wu, and Benoit Boulet. 2021.  Spatial-temporal residential short-term load forecasting via graph neural networks.  _IEEE Transactions on Smart Grid_ 12, 6 (2021), 5373â€“5384. 
  * Lin etÂ al. (2022b) Xin Lin, Ramon Zamora, CraigÂ A Baguley, and AnuragÂ K Srivastava. 2022b.  A hybrid short-term load forecasting approach for individual residential customer.  _IEEE Transactions on Power Delivery_ 38, 1 (2022), 26â€“37. 
  * Liu etÂ al. (2022c) Jiefeng Liu, Zhenhao Zhang, Xianhao Fan, Yiyi Zhang, Jiaqi Wang, Ke Zhou, Shuo Liang, Xiaoyong Yu, and Wei Zhang. 2022c.  Power system load forecasting using mobility optimization and multi-task learning in COVID-19.  _Applied Energy_ 310 (2022), 118303. 
  * Liu etÂ al. (2022a) Ronghui Liu, Teng Chen, Gaiping Sun, SM Muyeen, Shunfu Lin, and Yang Mi. 2022a.  Short-term probabilistic building load forecasting based on feature integrated artificial intelligent approach.  _Electric Power Systems Research_ 206 (2022), 107802. 
  * Liu etÂ al. (2022b) Yanzhu Liu, Shreya Dutta, Adams WaiÂ Kin Kong, and ChaiÂ Kiat Yeo. 2022b.  An image inpainting approach to short-term load forecasting.  _IEEE Transactions on Power Systems_ 38, 1 (2022), 177â€“187. 
  * Lu etÂ al. (2019) Jixiang Lu, Qipei Zhang, Zhihong Yang, and Mengfu Tu. 2019.  A hybrid model based on convolutional neural network and long short-term memory for short-term load forecasting. In _Proceedings of the 2019 IEEE Power & Energy Society General Meeting (PESGMâ€™19)_. 1â€“5. 
  * Lu etÂ al. (2022) Yuting Lu, Gaocai Wang, and Shuqiang Huang. 2022.  A short-term load forecasting model based on mixup and transfer learning.  _Electric Power Systems Research_ 207 (2022), 107837. 
  * Luo etÂ al. (2022) Hua Luo, Haipeng Zhang, and Jianzhou Wang. 2022.  Ensemble power load forecasting based on competitive-inhibition selection strategy and deep learning.  _Sustainable Energy Technologies and Assessments_ 51 (2022), 101940. 
  * Lv etÂ al. (2021) Lingling Lv, Zongyu Wu, Jinhua Zhang, Lei Zhang, Zhiyuan Tan, and Zhihong Tian. 2021.  A VMD and LSTM based hybrid model of load forecasting for power grid security.  _IEEE Transactions on Industrial Informatics_ 18, 9 (2021), 6474â€“6482. 
  * Mathew etÂ al. (2021) Jimson Mathew, RanjanÂ Kumar Behera, etÂ al. 2021\.  EMD-Att-LSTM: A data-driven strategy combined with deep learning for short-term load forecasting.  _Journal of Modern Power Systems and Clean Energy_ 10, 5 (2021), 1229â€“1240. 
  * Meng etÂ al. (2022) Zhaorui Meng, Yanqi Xie, and Jinhua Sun. 2022.  Short-term load forecasting using neural attention model based on EMD.  _Electrical Engineering_ (2022), 1â€“10. 
  * Morais etÂ al. (2023) Lucas BarrosÂ Scianni Morais, Giancarlo Aquila, Victor AugustoÂ DurÃ£es de Faria, Luana MedeirosÂ Marangon Lima, JosÃ© WanderleyÂ Marangon Lima, and AndersonÂ Rodrigo de Queiroz. 2023.  Short-term load forecasting using neural networks and global climate models: An application to a large-scale electrical power system.  _Applied Energy_ 348 (2023), 121439. 
  * Mounir etÂ al. (2023) Nada Mounir, Hamid Ouadi, and Ismael Jrhilifa. 2023.  Short-term electric load forecasting using an EMD-BI-LSTM approach for smart grid energy management system.  _Energy and Buildings_ 288 (2023), 113022. 
  * Mughees etÂ al. (2021) Neelam Mughees, SyedÂ Ali Mohsin, Abdullah Mughees, and Anam Mughees. 2021.  Deep sequence to sequence Bi-LSTM neural networks for day-ahead peak load forecasting.  _Expert Systems with Applications_ 175 (2021), 114844. 
  * Nawar etÂ al. (2023) Menna Nawar, Moustafa Shomer, Samy Faddel, and Huangjie Gong. 2023.  Transfer learning in deep learning models for building load forecasting: Case of limited data. In _Proceedings of the IEEE SoutheastCon 2023_. 532â€“538. 
  * Neeraj etÂ al. (2021) Neeraj Neeraj, Jimson Mathew, Mayank Agarwal, and RanjanÂ Kumar Behera. 2021.  Long short-term memory-singular spectrum analysis-based model for electric load forecasting.  _Electrical Engineering_ 103, 2 (2021), 1067â€“1082. 
  * Nie etÂ al. (2020) Ying Nie, Ping Jiang, and Haipeng Zhang. 2020.  A novel hybrid model based on combined preprocessing method and advanced optimization algorithm for power load forecasting.  _Applied Soft Computing_ 97 (2020), 106809. 
  * Niu etÂ al. (2016) Mingfei Niu, Shaolong Sun, Jing Wu, Lean Yu, and Jianzhou Wang. 2016.  An innovative integrated model using the singular spectrum analysis and nonlinear multi-layer perceptron network optimized by hybrid intelligent algorithm for short-term load forecasting.  _Applied Mathematical Modelling_ 40, 5-6 (2016), 4079â€“4093. 
  * Ozer etÂ al. (2021) Ilyas Ozer, SerhatÂ Berat Efe, and Harun Ozbay. 2021.  A combined deep learning application for short term load forecasting.  _Alexandria Engineering Journal_ 60, 4 (2021), 3807â€“3818. 
  * Park etÂ al. (1991) DongÂ C Park, MA El-Sharkawi, RJ Marks, LE Atlas, and MJ Damborg. 1991.  Electric load forecasting using an artificial neural network.  _IEEE Transactions on Power Systems_ 6, 2 (1991), 442â€“449. 
  * PeÅ‚ka and Dudek (2020) PaweÅ‚ PeÅ‚ka and Grzegorz Dudek. 2020.  Pattern-based long short-term memory for mid-term electrical load forecasting. In _Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNNâ€™20)_. 1â€“8. 
  * Qin etÂ al. (2022) Jiaqi Qin, Yi Zhang, Shixiong Fan, Xiaonan Hu, Yongqiang Huang, Zexin Lu, and Yan Liu. 2022.  Multi-task short-term reactive and active load forecasting method based on attention-LSTM model.  _International Journal of Electrical Power & Energy Systems_ 135 (2022), 107517. 
  * Rafati etÂ al. (2020) Amir Rafati, Mahmood Joorabian, and Elaheh Mashhour. 2020.  An efficient hour-ahead electrical load forecasting method based on innovative features.  _Energy_ 201 (2020), 117511. 
  * Ran etÂ al. (2023) Peng Ran, Kun Dong, Xu Liu, and Jing Wang. 2023.  Short-term load forecasting based on CEEMDAN and Transformer.  _Electric Power Systems Research_ 214 (2023), 108885. 
  * Raza etÂ al. (2017) MuhammadÂ Qamar Raza, Mithulananthan Nadarajah, DuongÂ Quoc Hung, and Zuhairi Baharudin. 2017.  An intelligent hybrid short-term load forecasting model for smart power grids.  _Sustainable Cities and Society_ 31 (2017), 264â€“275. 
  * Rumelhart etÂ al. (1986) DavidÂ E Rumelhart, GeoffreyÂ E Hinton, and RonaldÂ J Williams. 1986.  Learning representations by back-propagating errors.  _Nature_ 323, 6088 (1986), 533â€“536. 
  * Sadaei etÂ al. (2019) HosseinÂ Javedani Sadaei, PetrÃ´nio CÃ¢ndido deÂ Lima e Silva, FredericoÂ Gadelha Guimaraes, and MuhammadÂ Hisyam Lee. 2019.  Short-term load forecasting by using a combined method of convolutional neural networks and fuzzy time series.  _Energy_ 175 (2019), 365â€“377. 
  * Sakib etÂ al. (2021) Shadman Sakib, KhanÂ Md Hasib, IhtyazÂ Kader Tasawar, AbyazÂ Kader Tanzeem, MdÂ Fahim Arefin, Saharul Islam, and MohammadÂ Shafiul Alam. 2021.  A data-driven hybrid optimization based deep network model for short-term residential load forecasting. In _Proceedings of the IEEE 12th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCONâ€™21)_. 0187â€“0193. 
  * Santos etÂ al. (2023) MiguelÂ LÃ³pez Santos, SaÃºlÂ DÃ­az GarcÃ­a, Xela GarcÃ­a-Santiago, Ana Ogando-MartÃ­nez, FernandoÂ EchevarrÃ­a Camarero, GonzaloÂ BlÃ¡zquez Gil, and PabloÂ Carrasco Ortega. 2023.  Deep learning and transfer learning techniques applied to short-term load forecasting of data-poor buildings in local energy communities.  _Energy and Buildings_ 292 (2023), 113164. 
  * Schmidhuber (2015) JÃ¼rgen Schmidhuber. 2015.  Deep learning in neural networks: An overview.  _Neural networks_ 61 (2015), 85â€“117. 
  * Sekhar and Dahiya (2023) Charan Sekhar and Ratna Dahiya. 2023.  Robust framework based on hybrid deep learning approach for short term load forecasting of building electricity demand.  _Energy_ 268 (2023), 126660. 
  * Shaqour etÂ al. (2022) Ayas Shaqour, Tetsushi Ono, Aya Hagishima, and Hooman Farzaneh. 2022.  Electrical demand aggregation effects on the performance of deep learning-based short-term load forecasting of a residential building.  _Energy and AI_ 8 (2022), 100141. 
  * Sharma and Jain (2022) Abhishek Sharma and SachinÂ Kumar Jain. 2022.  A novel seasonal segmentation approach for day-ahead load forecasting.  _Energy_ 257 (2022), 124752. 
  * Shi etÂ al. (2023) Huifeng Shi, Kai Miao, and Xiaochen Ren. 2023.  Short-term load forecasting based on CNN-BiLSTM with Bayesian optimization and attention mechanism.  _Concurrency and Computation: Practice and Experience_ 35, 17 (2023), e6676. 
  * Singh and Dwivedi (2018) Priyanka Singh and Pragya Dwivedi. 2018.  Integration of new evolutionary approach with artificial neural network for solving short term load forecast problem.  _Applied energy_ 217 (2018), 537â€“549. 
  * Su etÂ al. (2023) Yongxin Su, Qiyao He, Jie Chen, and Mao Tan. 2023.  A residential load forecasting method for multi-attribute adversarial learning considering multi-source uncertainties.  _International Journal of Electrical Power & Energy Systems_ 154 (2023), 109421. 
  * Subbiah and Chinnappan (2022) SivaÂ Sankari Subbiah and Jayakumar Chinnappan. 2022.  Deep learning based short term load forecasting with hybrid feature selection.  _Electric Power Systems Research_ 210 (2022), 108065. 
  * Sun etÂ al. (2020) Gaiping Sun, Chuanwen Jiang, Xu Wang, and Xiu Yang. 2020.  Short-term building load forecast based on a data-mining feature selection and LSTM-RNN method.  _IEEJ Transactions on Electrical and Electronic Engineering_ 15, 7 (2020), 1002â€“1010. 
  * Sun etÂ al. (2018) Hongbin Sun, Xin Pan, and Changxin Meng. 2018.  A short-term power load prediction algorithm of based on power load factor deep cluster neural network.  _Wireless Personal Communications_ 102 (2018), 1073â€“1084. 
  * Tan etÂ al. (2022) Mao Tan, Chenglin Hu, Jie Chen, Ling Wang, and Zhengmao Li. 2022.  Multi-node load forecasting based on multi-task learning with modal feature extraction.  _Engineering Applications of Artificial Intelligence_ 112 (2022), 104856. 
  * Tang etÂ al. (2019b) Lingling Tang, Yulin Yi, and Yuexing Peng. 2019b.  An ensemble deep learning model for short-term load forecasting based on ARIMA and LSTM. In _Proceedings of the 2019 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridCommâ€™19)_. 1â€“6. 
  * Tang etÂ al. (2022) Xianlun Tang, Hongxu Chen, Wenhao Xiang, Jingming Yang, and Mi Zou. 2022.  Short-term load forecasting using channel and temporal attention based temporal convolutional network.  _Electric Power Systems Research_ 205 (2022), 107761. 
  * Tang etÂ al. (2019a) Xianlun Tang, Yuyan Dai, Ting Wang, and Yingjie Chen. 2019a.  Short-term power load forecasting based on multi-layer bidirectional recurrent neural network.  _IET Generation, Transmission & Distribution_ 13, 17 (2019), 3847â€“3854. 
  * Tayab etÂ al. (2020) UsmanÂ Bashir Tayab, Ali Zia, Fuwen Yang, Junwei Lu, and Muhammad Kashif. 2020.  Short-term load forecasting for microgrid energy management system using hybrid HHO-FNN model with best-basis stationary wavelet packet transform.  _Energy_ 203 (2020), 117857. 
  * Van DenÂ Oord etÂ al. (2016) Aaron Van DenÂ Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, etÂ al. 2016\.  Wavenet: A generative model for raw audio.  _arXiv preprint arXiv:1609.03499_ 12 (2016). 
  * VanÂ der Meer etÂ al. (2018) DennisÂ W VanÂ der Meer, Joakim WidÃ©n, and Joakim Munkhammar. 2018.  Review on probabilistic forecasting of photovoltaic power production and electricity consumption.  _Renewable and Sustainable Energy Reviews_ 81 (2018), 1484â€“1512. 
  * Vaswani etÂ al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanÂ N Gomez, Å�ukasz Kaiser, and Illia Polosukhin. 2017.  Attention is all you need.  _Advances in Neural Information Processing Systems_ 30 (2017). 
  * Veeramsetty etÂ al. (2022) Venkataramana Veeramsetty, DongariÂ Rakesh Chandra, Francesco Grimaccia, and Marco Mussetta. 2022.  Short term electric power load forecasting using principal component analysis and recurrent neural networks.  _Forecasting_ 4, 1 (2022), 149â€“164. 
  * VoÃŸ etÂ al. (2018) Marcus VoÃŸ, Christian Bender-Saebelkampf, and Sahin Albayrak. 2018.  Residential short-term load forecasting using convolutional neural networks. In _Proceedings of the 2018 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridCommâ€™18)_. 1â€“6. 
  * Wan etÂ al. (2023) Anping Wan, Qing Chang, AL-Bukhaiti Khalil, and Jiabo He. 2023.  Short-term power load forecasting for combined heat and power using CNN-LSTM enhanced by attention mechanism.  _Energy_ 282 (2023), 128274. 
  * Wang etÂ al. (2019b) Huaizhi Wang, Zhenxing Lei, Xian Zhang, Bin Zhou, and Jianchun Peng. 2019b.  A review of deep learning for renewable energy forecasting.  _Energy Conversion and Management_ 198 (2019), 111799. 
  * Wang etÂ al. (2023a) Jianguo Wang, Lincheng Han, Xiuyu Zhang, Yingzhou Wang, and Shude Zhang. 2023a.  Electrical load forecasting based on variable T-distribution and dual attention mechanism.  _Energy_ 283 (2023), 128569. 
  * Wang etÂ al. (2023c) Jianzhou Wang, Kang Wang, Zhiwu Li, Haiyan Lu, and He Jiang. 2023c.  Short-term power load forecasting system based on rough set, information granule and multi-objective optimization.  _Applied Soft Computing_ 146 (2023), 110692. 
  * Wang etÂ al. (2020b) Lingxiao Wang, Shiwen Mao, BogdanÂ M Wilamowski, and RM Nelms. 2020b.  Ensemble learning for load forecasting.  _IEEE Transactions on Green Communications and Networking_ 4, 2 (2020), 616â€“628. 
  * Wang etÂ al. (2023d) Lingyun Wang, Xiang Zhou, Honglei Xu, Tian Tian, and Huamin Tong. 2023d.  Short-term electrical load forecasting model based on multi-dimensional meteorological information spatio-temporal fusion and optimized variational mode decomposition.  _IET Generation, Transmission & Distribution_ 17, 20 (2023), 4647â€“4663. 
  * Wang etÂ al. (2021) Shouxiang Wang, Xinyu Deng, Haiwen Chen, Qingyuan Shi, and Di Xu. 2021.  A bottom-up short-term residential load forecasting approach based on appliance characteristic analysis and multi-task learning.  _Electric Power Systems Research_ 196 (2021), 107233. 
  * Wang etÂ al. (2019c) Shouxiang Wang, Xuan Wang, Shaomin Wang, and Dan Wang. 2019c.  Bi-directional long short-term memory method based on attention mechanism and rolling update for short-term load forecasting.  _International Journal of Electrical Power & Energy Systems_ 109 (2019), 470â€“479. 
  * Wang etÂ al. (2020a) Yuanyuan Wang, Jun Chen, Xiaoqiao Chen, Xiangjun Zeng, Yang Kong, Shanfeng Sun, Yongsheng Guo, and Ying Liu. 2020a.  Short-term load forecasting for industrial customers based on TCN-LightGBM.  _IEEE Transactions on Power Systems_ 36, 3 (2020), 1984â€“1997. 
  * Wang etÂ al. (2019a) Yi Wang, Dahua Gan, Mingyang Sun, Ning Zhang, Zongxiang Lu, and Chongqing Kang. 2019a.  Probabilistic individual load forecasting using pinball loss guided LSTM.  _Applied Energy_ 235 (2019), 10â€“20. 
  * Wang etÂ al. (2023b) Yufeng Wang, Lingxiao Rui, Jianhua Ma, etÂ al. 2023b.  A short-term residential load forecasting scheme based on the multiple correlation-temporal graph neural networks.  _Applied Soft Computing_ 146 (2023), 110629. 
  * Wu etÂ al. (2023a) Han Wu, Yan Liang, and Jiani Heng. 2023a.  Pulse-diagnosis-inspired multi-feature extraction deep network for short-term electricity load forecasting.  _Applied Energy_ 339 (2023), 120995. 
  * Wu etÂ al. (2023b) Kaitong Wu, Xiangang Peng, Zhiwen Chen, Haokun Su, Huan Quan, and Hanyu Liu. 2023b.  A novel short-term household load forecasting method combined BiLSTM with trend feature extraction.  _Energy Reports_ 9 (2023), 1013â€“1022. 
  * Wu etÂ al. (2021) Xuedong Wu, Yaonan Wang, Yingjie Bai, Zhiyu Zhu, and Aiming Xia. 2021.  Online short-term load forecasting methods using hybrids of single multiplicative neuron model, particle swarm optimization variants and nonlinear filters.  _Energy Reports_ 7 (2021), 683â€“692. 
  * Wu etÂ al. (2022) Zeqing Wu, Yunfei Mu, Shuai Deng, and Yang Li. 2022.  Spatialâ€“temporal short-term load forecasting framework via K-shape time series clustering method and graph convolutional networks.  _Energy Reports_ 8 (2022), 8752â€“8766. 
  * Xia etÂ al. (2023) Yurui Xia, Jianzhou Wang, Danxiang Wei, and Ziyuan Zhang. 2023.  Combined framework based on data preprocessing and multi-objective optimizer for electricity load forecasting.  _Engineering Applications of Artificial Intelligence_ 119 (2023), 105776. 
  * Xiang etÂ al. (2023) Siyu Xiang, Cao Zhen, Jian Peng, Linghao Zhang, and Zhengguo Pu. 2023.  Power load prediction of smart grid based on deep learning.  _Procedia Computer Science_ 228 (2023), 762â€“773. 
  * Xie etÂ al. (2022) Jiangjian Xie, Yujie Zhong, Tong Xiao, Zheng Wang, Junguo Zhang, Tuowai Wang, and BjÃ¶rnÂ W Schuller. 2022.  A multi-information fusion model for short term load forecasting of an architectural complex considering spatio-temporal characteristics.  _Energy and Buildings_ 277 (2022), 112566. 
  * Xu etÂ al. (2022) Lei Xu, Maomao Hu, and Cheng Fan. 2022.  Probabilistic electrical load forecasting for buildings using Bayesian deep neural networks.  _Journal of Building Engineering_ 46 (2022), 103853. 
  * Xu etÂ al. (2018) Liwen Xu, Chengdong Li, Xiuying Xie, and Guiqing Zhang. 2018.  Long-short-term memory network based hybrid model for short-term electrical load forecasting.  _Information_ 9, 7 (2018), 165. 
  * Yang etÂ al. (2023) Bo Yang, Xiaohui Yuan, and Fei Tang. 2023.  Iterative memory-driven load forecast network model for accuracy improvement.  _Energy Reports_ 9 (2023), 388â€“395. 
  * Yang etÂ al. (2022) Wangwang Yang, Jing Shi, Shujian Li, Zhaofang Song, Zitong Zhang, and Zexu Chen. 2022.  A combined deep learning load forecasting model of single household resident user considering multi-time scale electricity consumption behavior.  _Applied Energy_ 307 (2022), 118197. 
  * Yang etÂ al. (2019) Yandong Yang, Weijun Hong, and Shufang Li. 2019.  Deep ensemble learning based probabilistic load forecasting in smart grids.  _Energy_ 189 (2019), 116324. 
  * Yang etÂ al. (2018) Yandong Yang, Shufang Li, Wenqi Li, and Meijun Qu. 2018.  Power load probability density forecasting using gaussian process quantile regression.  _Applied Energy_ 213 (2018), 499â€“509. 
  * Yazici etÂ al. (2022) Ibrahim Yazici, OmerÂ Faruk Beyca, and Dursun Delen. 2022.  Deep-learning-based short-term electricity load forecasting: A real case application.  _Engineering Applications of Artificial Intelligence_ 109 (2022), 104645. 
  * Yi etÂ al. (2023) Shiyan Yi, Haichun Liu, Tao Chen, Jianwen Zhang, and Yibo Fan. 2023.  A deep LSTM-CNN based on self-attention mechanism with input data reduction for short-term load forecasting.  _IET Generation, Transmission & Distribution_ 17, 7 (2023), 1538â€“1552. 
  * Yin and Xie (2021) Linfei Yin and Jiaxing Xie. 2021.  Multi-temporal-spatial-scale temporal convolution network for short-term load forecasting of power systems.  _Applied Energy_ 283 (2021), 116328. 
  * Yu etÂ al. (2022) Fan Yu, Lei Wang, Qiaoyong Jiang, Qunmin Yan, and Shi Qiao. 2022.  Self-attention-based short-term load forecasting considering demand-side management.  _Energies_ 15, 12 (2022), 4198. 
  * Yu and Li (2021) Qun Yu and Zhiyi Li. 2021.  Correlated load forecasting in active distribution networks using spatial-temporal synchronous graph convolutional networks.  _IET Energy Systems Integration_ 3, 3 (2021), 355â€“366. 
  * Yu etÂ al. (2023) Xinli Yu, Zheng Chen, Yuan Ling, Shujing Dong, Zongyi Liu, and Yanbin Lu. 2023.  Temporal data meets LLMâ€“explainable financial time series forecasting.  _arXiv preprint arXiv:2306.11025_ (2023). 
  * Yue etÂ al. (2022) Weimin Yue, Qingrong Liu, Yingjun Ruan, Fanyue Qian, and Hua Meng. 2022.  A prediction approach with mode decomposition-recombination technique for short-term load forecasting.  _Sustainable Cities and Society_ 85 (2022), 104034. 
  * Zamee etÂ al. (2021) MuhammadÂ Ahsan Zamee, Dongjun Han, and Dongjun Won. 2021.  Online hour-ahead load forecasting using appropriate time-delay neural network based on multiple correlationâ€“multicollinearity analysis in IoT energy network.  _IEEE Internet of Things Journal_ 9, 14 (2021), 12041â€“12055. 
  * Zang etÂ al. (2021) Haixiang Zang, Ruiqi Xu, Lilin Cheng, Tao Ding, Ling Liu, Zhinong Wei, and Guoqiang Sun. 2021.  Residential load forecasting based on LSTM fusing self-attention mechanism with pooling.  _Energy_ 229 (2021), 120682. 
  * Zhang etÂ al. (2023e) Dongxue Zhang, Shuai Wang, Yuqiu Liang, and Zhiyuan Du. 2023e.  A novel combined model for probabilistic load forecasting based on deep learning and improved optimizer.  _Energy_ 264 (2023), 126172. 
  * Zhang etÂ al. (2022b) Guangqi Zhang, Chuyuan Wei, Changfeng Jing, and Yanxue Wang. 2022b.  Short-term electrical load forecasting based on time augmented transformer.  _International Journal of Computational Intelligence Systems_ 15, 1 (2022), 67:1â€“67:11. 
  * Zhang etÂ al. (2022a) Han Zhang, Chen Peng, Jun Li, Yajie Niu, and Longxiang Li. 2022a.  Electricity load forecasting based on an interpretable probsparse attention mechanism. In _Proceedings of the 3rd International Conference on Artificial Intelligence, Information Processing and Cloud Computing (AIIPCCâ€™22)_. 1â€“7. 
  * Zhang etÂ al. (2023d) Jinliang Zhang, Wang Siya, Tan Zhongfu, and Sun Anli. 2023d.  An improved hybrid model for short term power load prediction.  _Energy_ 268 (2023), 126561. 
  * Zhang etÂ al. (2021) Liang Zhang, Jin Wen, Yanfei Li, Jianli Chen, Yunyang Ye, Yangyang Fu, and William Livingood. 2021.  A review of machine learning in building load prediction.  _Applied Energy_ 285 (2021), 116452. 
  * Zhang etÂ al. (2023b) Qingyong Zhang, Jiahua Chen, Gang Xiao, Shangyang He, and Kunxiang Deng. 2023b.  TransformGraph: A novel short-term electricity net load forecasting model.  _Energy Reports_ 9 (2023), 2705â€“2717. 
  * Zhang etÂ al. (2022c) Ruixuan Zhang, Chuyan Zhang, and Miao Yu. 2022c.  A similar day based short term load forecasting method using wavelet transform and LSTM.  _IEEJ Transactions on Electrical and Electronic Engineering_ 17, 4 (2022), 506â€“513. 
  * Zhang etÂ al. (2023g) Ruixiang Zhang, Ziyu Zhu, Meng Yuan, Yihan Guo, Jie Song, Xuanxuan Shi, Yu Wang, and Yaojie Sun. 2023g.  Regional residential short-term load-interval forecasting based on SSA-LSTM and load consumption consistency analysis.  _Energies_ 16, 24 (2023), 8062. 
  * Zhang etÂ al. (2023a) Shiyun Zhang, Runhuan Chen, Jiacheng Cao, and Jian Tan. 2023a.  A CNN and LSTM-based multi-task learning architecture for short and medium-term electricity load forecasting.  _Electric power systems research_ 222 (2023), 109507. 
  * Zhang etÂ al. (2023f) Tingze Zhang, Xinan Zhang, TatÂ Kei Chau, Yau Chow, Tyrone Fernando, Herbert Ho-Ching Iu, etÂ al. 2023f.  Highly accurate peak and valley prediction short-term net load forecasting approach based on decomposition for power systems with high PV penetration.  _Applied Energy_ 333 (2023), 120641. 
  * Zhang etÂ al. (2023c) Zhenhao Zhang, Jiefeng Liu, Senshen Pang, Mingchen Shi, HuiÂ Hwang Goh, Yiyi Zhang, and Dongdong Zhang. 2023c.  General short-term load forecasting based on multi-task temporal convolutional network in COVID-19.  _International Journal of Electrical Power & Energy Systems_ 147 (2023), 108811. 
  * Zheng etÂ al. (2018) Jiaxiang Zheng, Xingying Chen, Kun Yu, Lei Gan, Yifan Wang, and Ke Wang. 2018.  Short-term power load forecasting of residential community based on GRU neural network. In _Proceedings of the 2018 International Conference on Power System Technology (POWERCONâ€™18)_. 4862â€“4868. 
  * Zhu etÂ al. (2022) Kedong Zhu, Yaping Li, Wenbo Mao, Feng Li, and Jiahao Yan. 2022.  LSTM enhanced by dual-attention-based encoder-decoder for daily peak load forecasting.  _Electric Power Systems Research_ 208 (2022), 107860. 
  * Zhuang etÂ al. (2022) Zhiyuan Zhuang, Xidong Zheng, Zixing Chen, and Tao Jin. 2022.  A reliable short-term power load forecasting method based on VMD-IWOA-LSTM algorithm.  _IEEJ Transactions on Electrical and Electronic Engineering_ 17, 8 (2022), 1121â€“1132. 


